{"cells":[{"cell_type":"markdown","metadata":{"id":"cLRenSd-xCPs"},"source":["# 입문용 이미지 캡셔닝(Image Captioning) 실습 노트북\n","\n","\n","- 데이터셋: GitHub에서 제공하는 **Flickr8k** 데이터셋 (일부 샘플만 사용)\n","- 인코더(Encoder): 사전 학습된 **ResNet-18(CNN)**\n","- 디코더(Decoder): **LSTM 기반 문장 생성기**\n","\n","\n"],"id":"cLRenSd-xCPs"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvO_5ZUwxCPt","outputId":"46fc6b9a-82fa-4139-bd9a-c7fee325780d","executionInfo":{"status":"ok","timestamp":1764832710674,"user_tz":-540,"elapsed":10265,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["사용 중인 디바이스: cuda\n"]}],"source":["# ===== 1. 기본 라이브러리 임포트 =====\n","import os  # 운영체제 기능(폴더 생성, 경로 처리 등)을 사용하기 위한 모듈\n","import re  # 정규표현식(텍스트 전처리)에 사용되는 모듈\n","import zipfile  # zip 파일(압축 파일)을 풀기 위해 사용하는 모듈\n","import random  # 무작위 샘플 추출, 시드 고정 등에 사용하는 모듈\n","from collections import Counter  # 단어 빈도수를 세기 위해 사용하는 자료구조\n","\n","import urllib.request  # 인터넷에서 파일을 다운로드하기 위한 표준 라이브러리 모듈\n","\n","import numpy as np  # 숫자 계산과 배열 연산을 편리하게 해주는 라이브러리\n","from PIL import Image  # 이미지 파일을 열고 다루기 위한 라이브러리(Pillow)\n","import matplotlib.pyplot as plt  # 그래프나 이미지를 화면에 출력하기 위한 라이브러리\n","\n","import torch  # PyTorch 딥러닝 프레임워크의 핵심 패키지\n","from torch import nn  # 신경망 레이어를 만들기 위한 모듈\n","from torch.utils.data import Dataset, DataLoader  # 데이터셋과 배치 생성을 도와주는 클래스들\n","\n","from torchvision import transforms  # 이미지 전처리(리사이즈, 텐서 변환 등)를 위한 모듈\n","from torchvision.models import resnet18, ResNet18_Weights  # 사전 학습된 ResNet-18 모델과 그 가중치 설정\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU가 있으면 GPU, 없으면 CPU를 사용하도록 설정\n","print(\"사용 중인 디바이스:\", device)  # 현재 사용 중인 디바이스를 출력하여 확인\n"],"id":"FvO_5ZUwxCPt"},{"cell_type":"code","metadata":{"id":"p-G_H02hxCPt","executionInfo":{"status":"ok","timestamp":1764832710684,"user_tz":-540,"elapsed":7,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":2,"outputs":[],"source":["# ===== 2. 재현성을 위한 시드(seed) 고정 =====\n","def set_seed(seed: int = 42):  # seed 값을 받아서 여러 라이브러리의 난수 발생기를 고정하는 함수 정의\n","    random.seed(seed)  # 파이썬 기본 random 모듈의 시드를 고정\n","    np.random.seed(seed)  # 넘파이의 난수 시드를 고정\n","    torch.manual_seed(seed)  # PyTorch CPU 난수 시드를 고정\n","    if torch.cuda.is_available():  # 만약 GPU(CUDA)가 사용 가능하다면\n","        torch.cuda.manual_seed_all(seed)  # 모든 GPU의 난수 시드를 고정\n","\n","set_seed(42)  # 위에서 정의한 함수를 호출하여 시드를 42로 고정\n"],"id":"p-G_H02hxCPt"},{"cell_type":"markdown","metadata":{"id":"TTsa9fstxCPt"},"source":["## 3. GitHub에서 Flickr8k 데이터 다운로드\n","\n","Flickr8k 데이터셋은 **이미지 캡션 연구**에 많이 사용되는 작은 이미지-문장 쌍 데이터셋입니다.\n","\n","여기서는 GitHub 저장소인\n","[`Avaneesh40585/Flickr8k-Dataset`](https://github.com/Avaneesh40585/Flickr8k-Dataset) 의\n","릴리즈에 올라온 **압축 파일(zip)** 을 직접 다운로드하여 사용합니다.\n"],"id":"TTsa9fstxCPt"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0RdNYd-xCPu","outputId":"11fd443e-0601-42f9-939e-ef4f02fbed01","executionInfo":{"status":"ok","timestamp":1764832747195,"user_tz":-540,"elapsed":36510,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["다운로드 중: https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n","완료: ./flickr8k/Flickr8k_Dataset.zip\n","다운로드 중: https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n","완료: ./flickr8k/Flickr8k_text.zip\n","압축 해제 중...\n","완료!\n","압축 해제 후 폴더 목록: ['Flickr8k_Dataset.zip', 'Flicker8k_Dataset', 'Flickr8k_text.zip', '__MACOSX']\n"]}],"source":["# ===== 3. Flickr8k 데이터 다운로드 및 압축 해제 =====\n","import os\n","import zipfile\n","import urllib.request\n","\n","# 전체 데이터셋이 들어갈 기본 폴더\n","data_dir = \"./flickr8k\"\n","os.makedirs(data_dir, exist_ok=True)\n","\n","# 정상 동작하는 Flickr8k 공식 미러 URL (Jason Brownlee GitHub)\n","images_zip_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n","text_zip_url   = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n","\n","images_zip_path = os.path.join(data_dir, \"Flickr8k_Dataset.zip\")\n","text_zip_path   = os.path.join(data_dir, \"Flickr8k_text.zip\")\n","\n","def download_if_not_exists(url, save_path):\n","    \"\"\"파일이 없을 때만 다운로드\"\"\"\n","    if not os.path.exists(save_path):\n","        print(f\"다운로드 중: {url}\")\n","        urllib.request.urlretrieve(url, save_path)\n","        print(f\"완료: {save_path}\")\n","    else:\n","        print(f\"이미 존재: {save_path}\")\n","\n","def unzip_if_needed(zip_path, extract_to):\n","    \"\"\"압축이 아직 안 풀려 있으면 압축 해제\"\"\"\n","    if not os.path.exists(extract_to):\n","        print(f\"압축 해제 중: {zip_path}\")\n","        with zipfile.ZipFile(zip_path, \"r\") as zf:\n","            zf.extractall(extract_to)\n","        print(f\"압축 해제 완료: {extract_to}\")\n","    else:\n","        print(f\"이미 압축 해제됨: {extract_to}\")\n","\n","# 1) zip 파일 다운로드\n","download_if_not_exists(images_zip_url, images_zip_path)\n","download_if_not_exists(text_zip_url,   text_zip_path)\n","\n","import zipfile\n","import os\n","\n","zip_path = \"/content/flickr8k/Flickr8k_Dataset.zip\"\n","extract_path = \"/content/flickr8k/\"\n","\n","print(\"압축 해제 중...\")\n","with zipfile.ZipFile(zip_path, \"r\") as zf:\n","    zf.extractall(extract_path)\n","\n","print(\"완료!\")\n","print(\"압축 해제 후 폴더 목록:\", os.listdir(extract_path))\n","\n","\n"],"id":"f0RdNYd-xCPu"},{"cell_type":"markdown","metadata":{"id":"ZfKI6SBtxCPu"},"source":["## 4. 캡션 파일 로드 및 구조 이해\n","\n","`Flickr8k.token.txt` 파일에는 **이미지 파일 이름과 그 이미지에 대한 여러 문장(캡션)** 이 함께 들어 있습니다.\n","\n","예시 형식:\n","\n","```text\n","1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\n","```\n","\n","- `1000268201_693b08cb0e.jpg` : 이미지 파일 이름\n","- `#0` : 이 이미지에 대한 0번째 캡션 (한 이미지당 5개의 캡션)\n","- 그 뒤 : 실제 문장\n"],"id":"ZfKI6SBtxCPu"},{"cell_type":"code","source":["import zipfile\n","\n","# 데이터셋 압축 파일 경로 (이미지에서 보인 경로를 기반으로 예시)\n","# 'data_dir'이 'flickr8k'의 상위 폴더를 가리킨다고 가정합니다.\n","zip_path = os.path.join(data_dir, \"Flickr8k_text.zip\")\n","\n","# 압축을 풀 디렉토리\n","extract_to_dir = data_dir\n","\n","# 압축 해제\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to_dir)\n","    print(f\"'{zip_path}' 파일이 '{extract_to_dir}'에 성공적으로 압축 해제되었습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s49DgjKDz3i5","outputId":"223be06b-2152-4446-802f-bc0fdd38c878","executionInfo":{"status":"ok","timestamp":1764832747255,"user_tz":-540,"elapsed":59,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"id":"s49DgjKDz3i5","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["'./flickr8k/Flickr8k_text.zip' 파일이 './flickr8k'에 성공적으로 압축 해제되었습니다.\n"]}]},{"cell_type":"code","source":["import os\n","\n","# 예시: data_dir이 'flickr8k'의 상위 디렉토리라고 가정\n","# 사용자 환경에 맞춰 'data_dir' 변수 설정이 필요합니다.\n","# 만약 'data_dir'이 이미 'flickr8k' 폴더를 가리킨다면, 아래 코드는 필요 없습니다.\n","# 여기서는 'flickr8k' 폴더 안에 데이터가 있다고 가정하고 경로를 설정합니다.\n","# **!!! 중요: 실제 환경에 맞게 data_dir 값을 설정해야 합니다 !!!**\n","# 예를 들어 data_dir이 '/content/' 이고 'flickr8k' 폴더가 그 안에 있다면:\n","# data_dir = '/content/flickr8k'\n","# 이 예시에서는 기존 코드의 'data_dir'이 'flickr8k' 폴더의 경로라고 가정하고 진행합니다.\n","\n","# --- 4. 캡션 파일 로드 ---\n","# 'Flickr8k_text.zip' 압축을 푼 후, 'Flickr8k.token.txt' 파일의 실제 경로를 확인하여 수정해야 합니다.\n","# 일반적으로 'data_dir' 안에 압축을 풀면 'Flickr8k.token.txt' 파일이 바로 생깁니다.\n","# 만약 에러가 발생했다면, 파일 이름이 잘못되었거나 경로가 잘못되었을 가능성이 큽니다.\n","\n","# 1. 파일 이름이 잘못되었을 경우 (혹시나 하여 오타 수정 가능성 포함):\n","# captions_file = os.path.join(data_dir, \"Flickr8k.token.txt\") # 기존 코드\n","\n","# 2. 파일이 'flickr8k' 폴더 안에 있고, data_dir이 'flickr8k'의 상위 폴더인 경우:\n","# data_dir이 'flickr8k' 폴더를 포함하는 상위 경로일 경우 아래처럼 수정해야 합니다.\n","# captions_file = os.path.join(data_dir, \"flickr8k\", \"Flickr8k.token.txt\")\n","# (이 경우 'data_dir'의 정확한 정의가 필요합니다.)\n","\n","# ***가장 일반적인 해결책: 파일 이름이 정확하다면, 압축을 풀지 않았거나 파일의 실제 위치가 다른 것입니다.***\n","\n","# **Flickr8k.token.txt 파일을 찾을 수 있는 올바른 경로로 수정하세요.**\n","# (예시: data_dir이 데이터를 담고 있는 최상위 폴더이고, 그 안에 'Flickr8k.token.txt'가 있다고 가정합니다.)\n","captions_file = os.path.join(data_dir, \"Flickr8k.token.txt\") # 압축을 푼 파일 경로를 지정\n","\n","print(\"캡션 파일 경로:\", captions_file)\n","\n","try:\n","    with open(captions_file, \"r\", encoding=\"utf-8\") as f:\n","        lines = f.readlines()\n","\n","    print(\"전체 캡션 라인 수:\", len(lines))\n","    print(\"앞에서 3줄만 미리 보기:\")\n","    for i in range(3):\n","        print(lines[i].strip())\n","\n","except Exception as e:\n","    print(f\"\\n파일을 읽는 중 예기치 않은 에러 발생: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hlef78uzxYb","outputId":"b4cd0535-c4fa-435d-a176-6c46c00fe9e7","executionInfo":{"status":"ok","timestamp":1764832747272,"user_tz":-540,"elapsed":11,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"id":"0hlef78uzxYb","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["캡션 파일 경로: ./flickr8k/Flickr8k.token.txt\n","전체 캡션 라인 수: 40460\n","앞에서 3줄만 미리 보기:\n","1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n","1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n","1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n"]}]},{"cell_type":"markdown","metadata":{"id":"_6Nn80LdxCPu"},"source":["## 5. 텍스트 전처리 및 이미지-캡션 매핑 만들기\n","\n","이미지 파일 이름별로 여러 개의 캡션 문장을 모아 두기 위해, 다음과 같은 과정을 거칩니다.\n","\n","1. 한 줄씩 읽어 **이미지 이름**과 **문장** 부분을 분리합니다.\n","2. 문장 안의 불필요한 기호(쉼표, 마침표 등)를 제거하고, 모두 소문자로 바꿉니다.\n","3. 이미지 이름을 key로 하고, 그 이미지에 대한 여러 캡션 리스트를 value로 갖는 딕셔너리를 만듭니다.\n"],"id":"_6Nn80LdxCPu"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfEpz87dxCPu","outputId":"bbc64ed1-df78-4963-e7ce-4fc89972c8c6","executionInfo":{"status":"ok","timestamp":1764832747634,"user_tz":-540,"elapsed":299,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["이미지 개수(캡션 포함): 8092\n","예시 이미지 파일 이름: 1000268201_693b08cb0e.jpg\n","해당 이미지의 캡션들:\n","- a child in a pink dress is climbing up a set of stairs in an entry way\n","- a girl going into a wooden building\n","- a little girl climbing into a wooden playhouse\n","- a little girl climbing the stairs to her playhouse\n","- a little girl in a pink dress going into a wooden cabin\n"]}],"source":["# ===== 5. 텍스트 전처리 및 이미지-캡션 딕셔너리 생성 =====\n","def clean_sentence(sentence: str) -> str:  # 한 문장을 깨끗하게 전처리하는 함수 정의\n","    sentence = sentence.lower()  # 모든 문자를 소문자로 변환 (예: 'A Dog' -> 'a dog')\n","    sentence = re.sub(r\"[^a-z ]\", \"\", sentence)  # 알파벳 소문자와 공백을 제외한 문자(숫자, 기호 등)를 제거\n","    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # 여러 개의 공백을 하나로 줄이고, 양끝 공백 제거\n","    return sentence  # 전처리가 끝난 문장을 반환\n","\n","captions_dict = {}  # 이미지 파일 이름을 key, 해당 이미지의 문장 리스트를 value로 저장할 딕셔너리\n","\n","for line in lines:  # 캡션 파일에서 읽어온 모든 줄을 하나씩 순회\n","    line = line.strip()  # 줄 끝의 줄바꿈 문자 등을 제거하여 깔끔한 문자열로 만듦\n","    if len(line) == 0:  # 빈 줄인 경우는 건너뛰기\n","        continue  # 다음 줄로 넘어감\n","    image_and_caption = line.split(\"\\t\")  # 탭 문자 기준으로 이미지 정보와 문장을 분리\n","    if len(image_and_caption) != 2:  # 만약 탭으로 나눈 결과가 2개가 아니라면 형식이 이상한 것이므로\n","        continue  # 해당 줄은 건너뛰고 다음 줄로 이동\n","    image_id_raw, caption_raw = image_and_caption  # 왼쪽은 이미지+번호, 오른쪽은 문장 부분으로 변수에 저장\n","    image_filename = image_id_raw.split(\"#\")[0]  # '파일이름#번호' 형태에서 앞부분(파일 이름)만 사용\n","    cleaned = clean_sentence(caption_raw)  # 위에서 정의한 함수로 문장을 전처리\n","    if len(cleaned.split()) < 3:  # 단어 수가 너무 적은 문장은 학습에 별 도움이 안 되므로 제외\n","        continue  # 다음 줄로 넘어감\n","    captions_dict.setdefault(image_filename, []).append(cleaned)  # 해당 이미지 파일 이름에 문장 추가\n","\n","print(\"이미지 개수(캡션 포함):\", len(captions_dict))  # 캡션이 있는 이미지가 몇 개인지 출력\n","\n","# 한 이미지에 어떤 캡션들이 들어 있는지 예시로 하나만 출력\n","sample_key = next(iter(captions_dict.keys()))  # 딕셔너리에서 임의의 첫 번째 key를 가져옴\n","print(\"예시 이미지 파일 이름:\", sample_key)  # 선택된 이미지 파일 이름 출력\n","print(\"해당 이미지의 캡션들:\")  # 그 이미지에 대응되는 문장들을 출력하겠다는 안내 메시지\n","for c in captions_dict[sample_key]:  # 선택된 이미지에 대한 캡션 리스트를 순회\n","    print(\"-\", c)  # 각 캡션을 한 줄에 하나씩 출력\n"],"id":"sfEpz87dxCPu"},{"cell_type":"markdown","metadata":{"id":"TD91Ce8-xCPu"},"source":["## 6. 입문용: 작은 서브셋(subset)만 사용하기\n","\n","실제 Flickr8k 데이터셋은 8,000장 이상의 이미지를 포함하지만, **수업 실습용** 으로는 너무 무겁습니다.\n","\n","그래서 여기서는 **임의로 200장의 이미지**만 선택해서, 간단한 모델을 빠르게 학습해 보겠습니다.\n"],"id":"TD91Ce8-xCPu"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvgRJFQsxCPu","outputId":"e4ad84b2-9f42-4ec3-9abb-8ee7c3b7aba2","executionInfo":{"status":"ok","timestamp":1764832747638,"user_tz":-540,"elapsed":5,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["캡션이 있는 전체 이미지 수: 8092\n","실습에 사용할 이미지 수: 200\n"]}],"source":["# ===== 6. 작은 서브셋 선택 =====\n","all_image_filenames = list(captions_dict.keys())  # 캡션이 있는 모든 이미지 파일 이름을 리스트로 변환\n","print(\"캡션이 있는 전체 이미지 수:\", len(all_image_filenames))  # 전체 이미지 개수를 출력\n","\n","subset_size = 200  # 서브셋으로 사용할 이미지 개수를 200장으로 설정\n","if len(all_image_filenames) < subset_size:  # 만약 전체 개수가 200보다 작다면\n","    subset_size = len(all_image_filenames)  # 사용할 개수를 전체 개수로 조정\n","\n","small_image_filenames = random.sample(all_image_filenames, subset_size)  # 전체 이미지 중에서 무작위로 subset_size개를 뽑음\n","print(\"실습에 사용할 이미지 수:\", len(small_image_filenames))  # 실제 사용할 이미지 개수 출력\n"],"id":"BvgRJFQsxCPu"},{"cell_type":"markdown","metadata":{"id":"upnom_VRxCPu"},"source":["## 7. 단어 사전(vocabulary) 만들기\n","\n","이미지 캡셔닝에서 문장을 다루려면, **단어를 숫자(index)** 로 바꾸는 과정이 필요합니다.\n","\n","- 특별 토큰(special token)\n","  - `<pad>` : 빈 자리를 채울 때 사용하는 토큰 (길이를 맞추기 위함)\n","  - `<start>` : 문장이 시작됨을 알리는 토큰\n","  - `<end>` : 문장이 끝났음을 알리는 토큰\n","  - `<unk>` : 사전에 없는 단어를 대신하는 토큰\n","\n","단어 빈도가 너무 낮은 단어는 모두 `<unk>`로 처리해, 사전의 크기를 적당히 줄입니다.\n"],"id":"upnom_VRxCPu"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBuCftf_xCPv","outputId":"bf3a8ef2-298e-4af7-fcd6-7d42ca5d782a","executionInfo":{"status":"ok","timestamp":1764832747650,"user_tz":-540,"elapsed":11,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["기준 이상으로 등장한 단어 수: 466\n","최종 단어 사전 크기: 470\n"]}],"source":["# ===== 7. 단어 사전 구성 =====\n","special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]  # 특별한 의미를 가지는 4개의 특수 토큰 리스트\n","\n","word_counter = Counter()  # 각 단어가 몇 번 등장했는지 세기 위한 Counter 객체\n","for img in small_image_filenames:  # 선택된 서브셋 이미지들에 대해서만 반복\n","    for cap in captions_dict[img]:  # 각 이미지에 대해 여러 캡션들을 순회\n","        for w in cap.split():  # 문장을 공백 기준으로 나누어 단어 리스트를 얻음\n","            word_counter[w] += 1  # 해당 단어의 등장 빈도를 1 증가시킴\n","\n","min_freq = 3  # 단어가 최소 몇 번 이상 나타나야 사전에 포함할지 기준 (여기서는 3번 이상)\n","vocab_words = [w for w, c in word_counter.items() if c >= min_freq]  # 등장 빈도가 기준 이상인 단어만 추려서 리스트 생성\n","print(\"기준 이상으로 등장한 단어 수:\", len(vocab_words))  # 사전에 포함될 일반 단어 수를 출력\n","\n","idx2word = []  # 인덱스에서 단어로 바꾸기 위한 리스트(인덱스 -> 단어)\n","idx2word.extend(special_tokens)  # 앞쪽에 특수 토큰들을 순서대로 추가\n","idx2word.extend(sorted(vocab_words))  # 나머지 단어들을 정렬하여 뒤에 붙임\n","\n","word2idx = {w: i for i, w in enumerate(idx2word)}  # 단어에서 인덱스로 바꾸기 위한 딕셔너리(단어 -> 인덱스)\n","\n","pad_idx = word2idx[\"<pad>\"]  # 패딩 토큰의 인덱스를 변수로 저장 (나중에 손실 계산에서 무시할 때 사용)\n","start_idx = word2idx[\"<start>\"]  # 문장 시작 토큰의 인덱스\n","end_idx = word2idx[\"<end>\"]  # 문장 끝 토큰의 인덱스\n","unk_idx = word2idx[\"<unk>\"]  # 사전에 없는 단어를 대신할 토큰의 인덱스\n","\n","vocab_size = len(idx2word)  # 최종 단어 사전의 크기(특수 토큰 포함)\n","print(\"최종 단어 사전 크기:\", vocab_size)  # 사전 크기를 출력\n"],"id":"MBuCftf_xCPv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlkUMG_pxCPv","outputId":"aec2067a-73b9-473d-a56e-e15690694fac","executionInfo":{"status":"ok","timestamp":1764832747657,"user_tz":-540,"elapsed":6,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["예시 원본 문장: there are two dogs in the snow and one has something in his mouth\n","숫자 시퀀스: [1, 411, 18, 436, 121, 198, 408, 369, 15, 2]\n","다시 단어로: ['<start>', 'there', 'are', 'two', 'dogs', 'in', 'the', 'snow', 'and', '<end>']\n"]}],"source":["# ===== 8. 문장을 숫자 시퀀스로 변환하는 함수 =====\n","def sentence_to_indices(sentence: str, max_len: int = 20):  # 문장과 최대 길이를 받아서 인덱스 리스트로 변환하는 함수\n","    tokens = sentence.split()  # 공백 기준으로 단어들을 분리하여 리스트로 만듦\n","    indices = [start_idx]  # 문장 시작을 의미하는 토큰 인덱스를 맨 앞에 추가\n","    for w in tokens:  # 문장의 각 단어에 대해 반복\n","        idx = word2idx.get(w, unk_idx)  # 단어가 사전에 있으면 그 인덱스를, 없으면 <unk> 인덱스를 가져옴\n","        indices.append(idx)  # 인덱스 리스트에 추가\n","        if len(indices) >= max_len - 1:  # 이미 충분히 길어졌다면 (마지막에 <end>를 하나 더 붙일 예정)\n","            break  # 더 이상 단어를 추가하지 않고 반복 종료\n","    indices.append(end_idx)  # 문장 끝을 의미하는 <end> 토큰 인덱스를 마지막에 추가\n","    # 길이가 너무 짧으면 뒤쪽을 <pad> 인덱스로 채워서 길이를 맞춤\n","    if len(indices) < max_len:  # 현재 길이가 최대 길이보다 짧다면\n","        indices.extend([pad_idx] * (max_len - len(indices)))  # 남은 부분을 모두 <pad>로 채움\n","    return indices  # 완성된 인덱스 리스트를 반환\n","\n","# 예시로 한 문장을 숫자 시퀀스로 변환해 보기\n","example_sentence = captions_dict[small_image_filenames[0]][0]  # 서브셋의 첫 번째 이미지에 대한 첫 번째 캡션 문장을 가져옴\n","print(\"예시 원본 문장:\", example_sentence)  # 원본 문장을 출력\n","example_indices = sentence_to_indices(example_sentence, max_len=10)  # 최대 길이를 10으로 제한하여 인덱스 시퀀스로 변환\n","print(\"숫자 시퀀스:\", example_indices)  # 변환된 인덱스 리스트를 출력\n","print(\"다시 단어로:\", [idx2word[i] for i in example_indices])  # 인덱스를 다시 단어로 바꿔서 사람이 읽을 수 있게 출력\n"],"id":"xlkUMG_pxCPv"},{"cell_type":"markdown","metadata":{"id":"ykoJuNC3xCPv"},"source":["## 8. PyTorch Dataset 만들기 (이미지 + 캡션)\n","\n","딥러닝 학습을 위해서는 데이터를 **(입력, 정답)** 형태로 계속 공급해 주어야 합니다.\n","\n","- 입력(Input): 전처리된 이미지 텐서\n","- 정답(Target): 같은 이미지에 대한 캡션 문장(숫자 시퀀스)\n","\n","PyTorch의 `Dataset` 클래스를 상속하여, 우리가 원하는 형식으로 데이터를 꺼낼 수 있도록 만들어 봅니다.\n"],"id":"ykoJuNC3xCPv"},{"cell_type":"code","metadata":{"id":"Mapw-FC1xCPv","executionInfo":{"status":"ok","timestamp":1764832747658,"user_tz":-540,"elapsed":1,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":10,"outputs":[],"source":["from torch.utils.data import Dataset\n","import os\n","\n","class Flickr8kDataset(Dataset):\n","    def __init__(self, image_folder, captions_dict, transform=None, max_len=20):\n","        self.image_folder = image_folder         # 예: \"./flickr8k/Flicker8k_Dataset\"\n","        self.captions_dict = captions_dict       # {\"파일명.jpg\": [token_id,...], ...}\n","        self.transform = transform\n","        self.max_len = max_len\n","\n","        # 1) 캡션에 등장하는 전체 이미지 파일명\n","        all_image_ids = list(captions_dict.keys())\n","\n","        # 2) 실제 폴더에 존재하는 파일만 남기기\n","        valid_image_ids = []\n","        missing_image_ids = []\n","\n","        for img_id in all_image_ids:\n","            img_path = os.path.join(self.image_folder, img_id)\n","            if os.path.exists(img_path):\n","                valid_image_ids.append(img_id)\n","            else:\n","                missing_image_ids.append(img_id)\n","\n","        self.image_ids = valid_image_ids\n","\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","\n","    def __getitem__(self, idx):\n","        img_id = self.image_ids[idx]\n","        img_path = os.path.join(self.image_folder, img_id)\n","\n","        # 혹시 모를 예외 상황 방지용 (거의 안 나오겠지만 안전장치)\n","        if not os.path.exists(img_path):\n","            raise FileNotFoundError(f\"Dataset 내부 오류: {img_path} 가 존재하지 않습니다.\")\n","\n","        # 이미지 로드\n","        from PIL import Image\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        # 캡션 텐서 가져오기 (이미 앞에서 토크나이즈 + 패딩했다고 가정)\n","        caption = self.captions_dict[img_id]\n","\n","        return image, caption\n"],"id":"Mapw-FC1xCPv"},{"cell_type":"markdown","metadata":{"id":"ckXZQi7vxCPv"},"source":["## 9. CNN 인코더(Encoder) 정의\n","\n","이미지 캡셔닝에서 **인코더(Encoder)** 는 이미지를 입력받아, 그 이미지의 특징을 요약한 **벡터(feature vector)** 를 만들어 줍니다.\n","\n","여기서는 **사전 학습된 ResNet-18** 모델을 사용하여, 마지막 분류 레이어 부분만 제거하고 **512차원 특징 벡터**를 사용합니다.\n"],"id":"ckXZQi7vxCPv"},{"cell_type":"code","metadata":{"id":"RCnOb9MjxCPv","executionInfo":{"status":"ok","timestamp":1764832747660,"user_tz":-540,"elapsed":1,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":11,"outputs":[],"source":["# ===== 11. CNN 인코더 정의 (ResNet-18) =====\n","class EncoderCNN(nn.Module):  # PyTorch의 nn.Module을 상속하여 이미지 인코더 클래스를 정의\n","    def __init__(self, embed_size: int = 256):  # 임베딩 차원(embed_size)을 인자로 받아 초기화\n","        super().__init__()  # 부모 클래스(nn.Module)의 초기화 메서드 호출\n","        weights = ResNet18_Weights.DEFAULT  # torchvision에서 제공하는 ResNet-18의 기본 사전 학습 가중치 설정\n","        resnet = resnet18(weights=weights)  # 사전 학습된 가중치를 가진 ResNet-18 모델 불러오기\n","        modules = list(resnet.children())[:-1]  # 마지막 분류용 FC 레이어를 제외한 나머지 레이어들만 리스트로 추출\n","        self.cnn = nn.Sequential(*modules)  # 추출한 레이어들을 nn.Sequential로 묶어서 하나의 모듈로 구성\n","        self.fc = nn.Linear(resnet.fc.in_features, embed_size)  # ResNet 마지막 특성 차원에서 embed_size로 줄이는 선형 레이어\n","        self.bn = nn.BatchNorm1d(embed_size)  # 학습 안정화를 위해 배치 정규화 레이어 추가\n","\n","        for param in self.cnn.parameters():  # 사전 학습된 CNN 가중치들에 대해 반복\n","            param.requires_grad = False  # 입문용 예제에서는 CNN 부분은 학습하지 않고 고정(freeze)하여 빠르게 학습\n","\n","    def forward(self, images):  # 순전파(forward) 메서드 정의, 입력은 이미지 텐서\n","        features = self.cnn(images)  # CNN을 통과시켜 (배치, 채널, 1, 1) 형태의 특징 맵을 얻음\n","        features = features.view(features.size(0), -1)  # (배치, 채널, 1, 1)을 (배치, 채널) 형태로 펼침\n","        features = self.fc(features)  # 선형 레이어를 통과시켜 embed_size 차원의 벡터로 변환\n","        features = self.bn(features)  # 배치 정규화로 분포를 안정화\n","        return features  # 최종 이미지 특징 벡터를 반환\n"],"id":"RCnOb9MjxCPv"},{"cell_type":"code","source":["# ===== 10. Image preprocessing (transform) and DataLoader definition =====\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# 1) 이미지 전처리 파이프라인 정의\n","image_transform = transforms.Compose([     # 여러 전처리(transform)를 순서대로 적용\n","    transforms.Resize((224, 224)),         # ResNet 입력 크기에 맞게 224x224로 리사이즈\n","    transforms.ToTensor(),                 # 이미지를 [0,1] 범위의 텐서(채널, 높이, 너비)로 변환\n","    transforms.Normalize(                  # ImageNet 통계값으로 정규화\n","        mean=[0.485, 0.456, 0.406],        # 채널별 평균 (ImageNet)\n","        std=[0.229, 0.224, 0.225],         # 채널별 표준편차 (ImageNet)\n","    ),\n","])\n","\n","max_caption_len = 20  # 캡션의 최대 길이를 20 단어로 제한\n","images_folder = \"/content/flickr8k/Flicker8k_Dataset\"\n","print(os.path.exists(images_folder))  # True 가 나와야 정상\n","\n","\n","\n","# 2) Flickr8k 커스텀 데이터셋 생성\n","dataset = Flickr8kDataset(\n","    image_folder=images_folder,\n","    captions_dict=captions_dict,\n","    transform=image_transform,\n","    max_len=max_caption_len,\n",")\n","\n","print(\"Number of (image, caption) samples:\", len(dataset))  # 6000~8000 사이\n","\n","\n","# 3) DataLoader 정의\n","batch_size = 16  # 한 번에 모델에 넣을 배치 크기\n","\n","dataloader = DataLoader(\n","    dataset,\n","    batch_size=batch_size,  # ← 세미콜론(;)이 아니라 쉼표(,) 사용\n","    shuffle=True,\n","    num_workers=2,          # 데이터 로딩에 사용할 워커 수 (Colab이면 2~4 정도 권장)\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7tLanSzYvLVk","outputId":"c73eb1cf-62f7-4e2f-e6bf-85ed58e68c80","executionInfo":{"status":"ok","timestamp":1764832747674,"user_tz":-540,"elapsed":7,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"id":"7tLanSzYvLVk","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","Number of (image, caption) samples: 8091\n"]}]},{"cell_type":"markdown","metadata":{"id":"IhQwsdJdxCPv"},"source":["## 10. LSTM 디코더(Decoder) 정의\n","\n","디코더는 인코더가 만든 **이미지 특징 벡터**와 이전까지 생성된 단어들을 이용하여,\n","다음 단어를 하나씩 예측하는 **문장 생성기**입니다.\n","\n","1. 단어를 **임베딩(Embedding) 레이어**를 통해 숫자 벡터로 바꾼 뒤,\n","2. **LSTM** 에 순서대로 넣어 주고,\n","3. LSTM의 출력을 **Linear 레이어**를 통해 각 단어가 나올 확률로 변환합니다.\n"],"id":"IhQwsdJdxCPv"},{"cell_type":"code","metadata":{"id":"LerD8vn2xCPv","executionInfo":{"status":"ok","timestamp":1764832747685,"user_tz":-540,"elapsed":9,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":13,"outputs":[],"source":["# ===== 12. LSTM 디코더 정의 =====\n","class DecoderRNN(nn.Module):  # PyTorch nn.Module을 상속하여 디코더 클래스를 정의\n","    def __init__(self, embed_size: int, hidden_size: int, vocab_size: int, num_layers: int = 1):  # 초기화 메서드\n","        super().__init__()  # 부모 클래스 초기화\n","        self.embed = nn.Embedding(vocab_size, embed_size)  # 단어 인덱스를 embed_size 차원의 벡터로 바꿔주는 임베딩 레이어\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        # LSTM 레이어 정의 (입력: embed_size, 은닉: hidden_size)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        # LSTM 출력을 단어 사전 크기만큼의 로짓(logit)으로 변환하는 선형 레이어\n","\n","    def forward(self, features, captions):  # 순전파 메서드, features: 이미지 벡터, captions: 정답 캡션 시퀀스\n","        embeddings = self.embed(captions)  # (배치, 시퀀스 길이) 형태의 캡션 인덱스를 임베딩 벡터로 변환\n","        features = features.unsqueeze(1)\n","        # (배치, embed_size)를 (배치, 1, embed_size)로 차원 확장하여 LSTM 첫 입력으로 사용\n","        inputs = torch.cat((features, embeddings[:, :-1, :]), dim=1)\n","        # 이미지 특징 뒤에 캡션의 마지막 토큰을 제외한 부분을 이어붙여 입력 시퀀스 생성\n","        outputs, _ = self.lstm(inputs)  # LSTM에 입력 시퀀스를 넣어 전체 시퀀스에 대한 은닉 상태 출력\n","        outputs = self.fc(outputs)  # 각 시점의 LSTM 출력을 단어 사전 크기의 로짓으로 변환\n","        return outputs  # (배치, 시퀀스 길이, vocab_size) 형태의 예측 결과 반환\n","\n","    def sample(self, features, max_len=20):  # 학습된 모델로부터 실제 문장을 생성하기 위한 메서드\n","        generated_indices = []  # 생성된 단어 인덱스를 순서대로 저장할 리스트\n","        inputs = features.unsqueeze(1)  # (배치=1, 1, embed_size) 형태로 LSTM 입력 준비\n","        states = None  # LSTM의 초기 은닉 상태와 셀 상태는 None으로 두면 자동 초기화\n","        for _ in range(max_len):  # 최대 max_len 길이만큼 단어를 생성\n","            outputs, states = self.lstm(inputs, states)  # 현재 입력과 상태를 LSTM에 넣어 한 시점의 출력을 얻음\n","            outputs = self.fc(outputs.squeeze(1))  # LSTM 출력을 선형 레이어에 통과시켜 단어별 로짓으로 변환\n","            _, predicted = outputs.max(1)  # 가장 확률이 높은 단어 인덱스를 선택\n","            generated_indices.append(predicted.item())  # 선택된 인덱스를 리스트에 추가\n","            if predicted.item() == end_idx:  # 만약 <end> 토큰이 나오면 문장 생성을 멈춤\n","                break  # 반복문 종료\n","            inputs = self.embed(predicted).unsqueeze(1)  # 예측된 단어를 임베딩하여 다음 시점의 입력으로 사용\n","        return generated_indices  # 생성된 단어 인덱스 리스트를 반환\n"],"id":"LerD8vn2xCPv"},{"cell_type":"code","metadata":{"id":"MmHV_qhBxCPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764832748543,"user_tz":-540,"elapsed":857,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"af771442-ec38-49c4-eacf-1ff258781d7c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 44.7M/44.7M [00:00<00:00, 241MB/s]\n"]}],"source":["# ===== 13. 모델 인스턴스 생성 및 학습 설정 =====\n","embed_size = 256  # 이미지 특징 벡터와 단어 임베딩 벡터의 차원을 256으로 설정\n","hidden_size = 512  # LSTM 은닉 상태의 차원을 512로 설정\n","\n","encoder = EncoderCNN(embed_size=embed_size).to(device)  # EncoderCNN 인스턴스를 만들고, GPU/CPU 디바이스로 이동\n","decoder = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size).to(device)  # DecoderRNN 인스턴스를 만들고 디바이스로 이동\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)  # 손실 함수로 다중 클래스 분류에 사용하는 CrossEntropyLoss를 사용, 패딩 토큰은 무시\n","params = list(decoder.parameters()) + [p for p in encoder.fc.parameters()] + [p for p in encoder.bn.parameters()]\n","# 학습할 파라미터들만 모아서 리스트로 생성\n","optimizer = torch.optim.Adam(params, lr=1e-3)  # Adam 옵티마이저를 사용하여 파라미터를 업데이트, 학습률은 0.001로 설정\n"],"id":"MmHV_qhBxCPv"},{"cell_type":"markdown","metadata":{"id":"Sevxk9J1xCPv"},"source":["## 11. 간단한 학습 루프 (입문용으로 1~2 epoch만 돌려보기)\n","\n","실제 연구에서는 수십 epoch 동안 대량의 데이터를 학습시키지만, 여기서는 **개념 이해**와 **코드 흐름 익히기**가 목적이므로\n","아주 적은 epoch만 학습시킵니다.\n"],"id":"Sevxk9J1xCPv"},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import os\n","\n","class Flickr8kDataset(Dataset):\n","    def __init__(self, image_folder, captions_dict, transform=None, max_len=20):\n","        self.image_folder = image_folder         # 예: \"./flickr8k/Flicker8k_Dataset\"\n","        self.captions_dict = captions_dict       # {\"파일명.jpg\": [token_id,...], ...}\n","        self.transform = transform\n","        self.max_len = max_len\n","\n","        # 1) 캡션에 등장하는 전체 이미지 파일명\n","        all_image_ids = list(captions_dict.keys())\n","\n","        # 2) 실제 폴더에 존재하는 파일만 남기기\n","        valid_image_ids = []\n","        missing_image_ids = []\n","\n","        for img_id in all_image_ids:\n","            img_path = os.path.join(self.image_folder, img_id)\n","            if os.path.exists(img_path):\n","                valid_image_ids.append(img_id)\n","            else:\n","                missing_image_ids.append(img_id)\n","\n","        self.image_ids = valid_image_ids\n","\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","\n","    def __getitem__(self, idx):\n","        img_id = self.image_ids[idx]\n","        img_path = os.path.join(self.image_folder, img_id)\n","\n","        # 혹시 모를 예외 상황 방지용 (거의 안 나오겠지만 안전장치)\n","        if not os.path.exists(img_path):\n","            raise FileNotFoundError(f\"Dataset 내부 오류: {img_path} 가 존재하지 않습니다.\")\n","\n","        # 이미지 로드\n","        from PIL import Image\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        # 캡션 텐서 가져오기 (이미 앞에서 토크나이즈 + 패딩했다고 가정)\n","        caption = self.captions_dict[img_id]\n","\n","        return image, caption\n"],"metadata":{"id":"4F4Lv5F5t6dp","executionInfo":{"status":"ok","timestamp":1764832748555,"user_tz":-540,"elapsed":2,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"id":"4F4Lv5F5t6dp","execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pQerL5YxCPw"},"source":["## 12. 학습된 모델로 캡션 생성해 보기\n","\n","학습이 끝난 후, 몇 개의 이미지를 골라 **모델이 어떤 문장을 만들어 내는지** 직접 확인해 봅니다.\n"],"id":"6pQerL5YxCPw"},{"cell_type":"code","source":["# ===== 4. 캡션 파일 로드 =====\n","# 위 셀에서 정의한 captions_file 변수를 그대로 사용합니다.\n","\n","print(\"캡션 파일 경로:\", captions_file)\n","\n","try:\n","    # Flickr8k.token.txt 전체 읽기\n","    with open(captions_file, \"r\", encoding=\"utf-8\") as f:\n","        lines = f.readlines()\n","\n","    print(\"전체 캡션 라인 수:\", len(lines))\n","    print(\"앞에서 3줄 미리 보기:\")\n","    for i in range(3):\n","        print(lines[i].strip())\n","\n","except Exception as e:\n","    print(f\"\\n캡션 파일을 읽는 중 예기치 않은 에러 발생: {e}\")\n","    raise\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50fP2_gTsvjq","outputId":"a6ffbd50-c5c5-412c-954a-e7df87e568a3","executionInfo":{"status":"ok","timestamp":1764832748568,"user_tz":-540,"elapsed":12,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"id":"50fP2_gTsvjq","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["캡션 파일 경로: ./flickr8k/Flickr8k.token.txt\n","전체 캡션 라인 수: 40460\n","앞에서 3줄 미리 보기:\n","1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n","1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n","1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n"]}]},{"cell_type":"markdown","metadata":{"id":"xB7j89MoxCPw"},"source":["## 13. 정리\n","\n","1. GitHub에서 Flickr8k 데이터셋을 다운로드하고, 이미지와 캡션을 로드했습니다.\n","2. 텍스트를 전처리하여 단어 사전(vocabulary)을 만들고, 문장을 숫자 시퀀스로 변환했습니다.\n","3. 사전 학습된 ResNet-18을 인코더로 사용해 이미지 특징을 추출했습니다.\n","4. LSTM 기반 디코더를 사용해, 이미지 특징과 캡션을 이용하여 다음 단어를 예측하는 모델을 만들었습니다.\n","5. 작은 서브셋에 대해 간단한 학습을 수행하고, 실제로 캡션을 생성해 보았습니다.\n","\n"],"id":"xB7j89MoxCPw"},{"cell_type":"code","source":["# eos"],"metadata":{"id":"5HJ8lGB1YU6M","executionInfo":{"status":"ok","timestamp":1764832748570,"user_tz":-540,"elapsed":1,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"id":"5HJ8lGB1YU6M","execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HhUzGsi_YhKR","executionInfo":{"status":"ok","timestamp":1764832748573,"user_tz":-540,"elapsed":2,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"id":"HhUzGsi_YhKR","execution_count":17,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10"},"colab":{"provenance":[],"gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}