{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNFv4tJ9vnae/0UE8iSF59z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 입문용 이미지 캡셔닝(Image Captioning) 실습 노트북\n","\n","이 노트북에서는 **CNN + LSTM 구조**를 이용해 간단한 이미지 캡셔닝 모델을 만들어 봅니다.\n","\n","- 데이터셋: GitHub에서 제공하는 **Flickr8k** 데이터셋 (일부 샘플만 사용)\n","- 인코더(Encoder): 사전 학습된 **ResNet-18(CNN)**\n","- 디코더(Decoder): **LSTM 기반 문장 생성기**"],"metadata":{"id":"WnX9pRQI4AY2"}},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U148xA4937g-","executionInfo":{"status":"ok","timestamp":1764666394465,"user_tz":-540,"elapsed":25,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"0052931f-58c7-4c17-b191-f233b7c2006e"},"outputs":[{"output_type":"stream","name":"stdout","text":["사용 중인 디바이스: cuda\n"]}],"source":["# ===== 1. 기본 라이브러리 임포트 =====\n","import os  # 운영체제 기능(폴더 생성, 경로 처리 등)을 사용하기 위한 모듈\n","import re  # 정규표현식(텍스트 전처리)에 사용되는 모듈\n","import zipfile  # zip 파일(압축 파일)을 풀기 위해 사용하는 모듈\n","import random  # 무작위 샘플 추출, 시드 고정 등에 사용하는 모듈\n","from collections import Counter  # 단어 빈도수를 세기 위해 사용하는 자료구조\n","\n","import urllib.request  # 인터넷에서 파일을 다운로드하기 위한 표준 라이브러리 모듈\n","\n","import numpy as np  # 숫자 계산과 배열 연산을 편리하게 해주는 라이브러리\n","from PIL import Image  # 이미지 파일을 열고 다루기 위한 라이브러리(Pillow)\n","import matplotlib.pyplot as plt  # 그래프나 이미지를 화면에 출력하기 위한 라이브러리\n","\n","import torch  # PyTorch 딥러닝 프레임워크의 핵심 패키지\n","from torch import nn  # 신경망 레이어를 만들기 위한 모듈\n","from torch.utils.data import Dataset, DataLoader  # 데이터셋과 배치 생성을 도와주는 클래스들\n","\n","from torchvision import transforms  # 이미지 전처리(리사이즈, 텐서 변환 등)를 위한 모듈\n","from torchvision.models import resnet18, ResNet18_Weights  # 사전 학습된 ResNet-18 모델과 그 가중치 설정\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU가 있으면 GPU, 없으면 CPU를 사용하도록 설정\n","print(\"사용 중인 디바이스:\", device)  # 현재 사용 중인 디바이스를 출력하여 확인\n"]},{"cell_type":"code","source":["# ===== 2. 재현성을 위한 시드(seed) 고정 =====\n","def set_seed(seed: int = 42):  # seed 값을 받아서 여러 라이브러리의 난수 발생기를 고정하는 함수 정의\n","    random.seed(seed)  # 파이썬 기본 random 모듈의 시드를 고정\n","    np.random.seed(seed)  # 넘파이의 난수 시드를 고정\n","    torch.manual_seed(seed)  # PyTorch CPU 난수 시드를 고정\n","    if torch.cuda.is_available():  # 만약 GPU(CUDA)가 사용 가능하다면\n","        torch.cuda.manual_seed_all(seed)  # 모든 GPU의 난수 시드를 고정\n","\n","set_seed(42)  # 위에서 정의한 함수를 호출하여 시드를 42로 고정"],"metadata":{"id":"U0WgTuKe4XeS","executionInfo":{"status":"ok","timestamp":1764666394470,"user_tz":-540,"elapsed":3,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":["## 3. GitHub에서 Flickr8k 데이터 다운로드\n"],"metadata":{"id":"Sv3gRJlO4mY4"}},{"cell_type":"code","source":["import os\n","import urllib.request # urllib.request가 정의되지 않았으므로 추가\n","import zipfile # zipfile이 정의되지 않았으므로 추가\n","\n","# ===== 3. GitHub에서 Flickr8k 데이터 다운로드 및 압축 해제 =====\n","data_dir = \"./flickr8k\"  # 전체 데이터셋을 저장할 기본 폴더 경로를 문자열로 지정\n","os.makedirs(data_dir, exist_ok=True)  # 위에서 지정한 폴더가 없으면 새로 만들고, 있으면 그대로 둠\n","\n","# GitHub 릴리즈에서 제공되는 이미지(zip)와 캡션 텍스트(zip)의 URL\n","images_zip_url = \"https://github.com/Avaneesh40585/Flickr8k-Dataset/releases/download/v1.0/Flickr8k_Dataset.zip\"  # 이미지 압축 파일의 인터넷 주소\n","text_zip_url = \"https://github.com/Avaneesh40585/Flickr8k-Dataset/releases/download/v1.0/Flickr8k_text.zip\"  # 캡션 텍스트 압축 파일의 인터넷 주소\n","\n","images_zip_path = os.path.join(data_dir, \"Flickr8k_Dataset.zip\")  # 이미지 zip 파일을 로컬에 저장할 경로\n","text_zip_path = os.path.join(data_dir, \"Flickr8k_text.zip\")  # 텍스트 zip 파일을 로컬에 저장할 경로\n","\n","def download_if_not_exists(url, save_path):  # 파일이 없을 때만 다운로드하는 함수 정의\n","    if not os.path.exists(save_path):  # 지정한 경로에 파일이 존재하지 않는다면\n","        print(f\"다운로드 중: {url}\")  # 어떤 URL을 다운로드 중인지 출력\n","        urllib.request.urlretrieve(url, save_path)  # urlretrieve 함수를 사용하여 URL에서 파일을 받아 로컬에 저장\n","        print(\"완료:\", save_path)  # 다운로드가 끝나면 완료 메시지 출력\n","    else:\n","        print(\"이미 존재함:\", save_path)  # 이미 파일이 있다면 다운로드를 생략하고 메시지만 출력\n","\n","download_if_not_exists(images_zip_url, images_zip_path)  # Flickr8k 이미지 zip 파일을 다운로드(필요할 때만)\n","download_if_not_exists(text_zip_url, text_zip_path)  # Flickr8k 텍스트 zip 파일을 다운로드(필요할 때만)\n","\n","# zip 파일을 실제 폴더로 압축 해제하는 함수 정의\n","def unzip_if_needed(zip_path, extract_to):  # zip 파일 경로와 압축을 풀 폴더 경로를 인자로 받음\n","    # 압축 해제된 폴더의 예상 이름을 미리 확인\n","    expected_folder_name = os.path.splitext(os.path.basename(zip_path))[0]\n","    expected_extract_path = os.path.join(extract_to, expected_folder_name)\n","\n","    # 압축 해제된 폴더가 아직 없다면\n","    if not os.path.exists(expected_extract_path):\n","        print(f\"압축 해제 중: {zip_path}\")  # 어느 zip 파일을 풀고 있는지 출력\n","        with zipfile.ZipFile(zip_path, \"r\") as zf:  # zipfile.ZipFile을 사용해 zip 파일을 읽기 모드로 엶\n","            zf.extractall(extract_to)  # 지정한 폴더로 모든 파일을 압축 해제\n","        print(\"압축 해제 완료:\", expected_extract_path)  # 완료 메시지 출력\n","    else:\n","        print(\"이미 압축 해제됨:\", expected_extract_path)  # 이미 폴더가 있다면 다시 풀지 않고 메시지만 출력\n","\n","# 압축 해제는 data_dir (./flickr8k)에 이루어지므로,\n","# 실제 데이터셋 폴더는 ./flickr8k/Flickr8k_Dataset 에 위치하게 됩니다.\n","unzip_if_needed(images_zip_path, data_dir)  # 이미지 zip 파일을 flickr8k 폴더 아래에 압축 해제\n","unzip_if_needed(text_zip_path, data_dir)  # 텍스트 zip 파일을 flickr8k 폴더 아래에 압축 해제\n","\n","images_folder = os.path.join(\"Flickr8k_Dataset\")\n","print(\"이미지 폴더 존재 여부:\", os.path.exists(images_folder))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7igxZ9BsRKjU","executionInfo":{"status":"ok","timestamp":1764666645156,"user_tz":-540,"elapsed":5805,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"57724cfa-bb08-4878-efc9-081c4778f663"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["이미 존재함: ./flickr8k/Flickr8k_Dataset.zip\n","이미 존재함: ./flickr8k/Flickr8k_text.zip\n","압축 해제 중: ./flickr8k/Flickr8k_Dataset.zip\n","압축 해제 완료: ./flickr8k/Flickr8k_Dataset\n","압축 해제 중: ./flickr8k/Flickr8k_text.zip\n","압축 해제 완료: ./flickr8k/Flickr8k_text\n","이미지 폴더 존재 여부: False\n"]}]},{"cell_type":"markdown","source":["## 4. 캡션 파일 로드 및 구조 이해"],"metadata":{"id":"B1jSLh_-5bi3"}},{"cell_type":"code","source":["import zipfile\n","\n","# 데이터셋 압축 파일 경로 (이미지에서 보인 경로를 기반으로 예시)\n","# 'data_dir'이 'flickr8k'의 상위 폴더를 가리킨다고 가정합니다.\n","zip_path = os.path.join(data_dir, \"Flickr8k_text.zip\")\n","\n","# 압축을 풀 디렉토리\n","extract_to_dir = data_dir\n","\n","# 압축 해제\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to_dir)\n","    print(f\"'{zip_path}' 파일이 '{extract_to_dir}'에 성공적으로 압축 해제되었습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c18ovYtx5d2W","executionInfo":{"status":"ok","timestamp":1764666650974,"user_tz":-540,"elapsed":60,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"79680973-4e38-47da-dc4d-1172c38bfa97"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["'./flickr8k/Flickr8k_text.zip' 파일이 './flickr8k'에 성공적으로 압축 해제되었습니다.\n"]}]},{"cell_type":"markdown","source":["## 5. 텍스트 전처리 및 이미지-캡션 매핑 만들기\n","\n","이미지 파일 이름별로 여러 개의 캡션 문장을 모아 두기 위해, 다음과 같은 과정을 거칩니다.\n","\n","1. 한 줄씩 읽어 **이미지 이름**과 **문장** 부분을 분리합니다.\n","2. 문장 안의 불필요한 기호(쉼표, 마침표 등)를 제거하고, 모두 소문자로 바꿉니다.\n","3. 이미지 이름을 key로 하고, 그 이미지에 대한 여러 캡션 리스트를 value로 갖는 딕셔너리를 만듭니다."],"metadata":{"id":"fKTWZSnF6Eo0"}},{"cell_type":"code","source":["# ===== 5. 텍스트 전처리 및 이미지-캡션 딕셔너리 생성 =====\n","def clean_sentence(sentence: str) -> str:  # 한 문장을 깨끗하게 전처리하는 함수 정의\n","    sentence = sentence.lower()  # 모든 문자를 소문자로 변환 (예: 'A Dog' -> 'a dog')\n","    sentence = re.sub(r\"[^a-z ]\", \"\", sentence)  # 알파벳 소문자와 공백을 제외한 문자(숫자, 기호 등)를 제거\n","    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # 여러 개의 공백을 하나로 줄이고, 양끝 공백 제거\n","    return sentence  # 전처리가 끝난 문장을 반환\n"],"metadata":{"id":"YJSiArAQ50vk","executionInfo":{"status":"ok","timestamp":1764666650980,"user_tz":-540,"elapsed":8,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":["# 테스트 = \"I love you. baby!^^6\"\n","# # 테스트\n","# 테스트.lower()\n","# re.sub(r\"[^a-z ]\", \"\", 테스트)\n","# re.sub(r\"\\s+\", \" \", 테스트).strip()\n","# clean_sentence(테스트)"],"metadata":{"id":"P8lPEvbZ6pHm","executionInfo":{"status":"ok","timestamp":1764666650984,"user_tz":-540,"elapsed":7,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["file_path = '/content/flickr8k/Flickr8k.token.txt'\n","# /content/flickr8k/Flickr8k.token.txt\n","\n","with open(file_path, 'r', encoding='utf-8') as f:\n","    lines = f.readlines()\n","\n","print(len(lines))\n","\n","captions_dict = {}  # 이미지 파일 이름을 key, 해당 이미지의 문장 리스트를 value로 저장할 딕셔너리\n","\n","for line in lines:  # 캡션 파일에서 읽어온 모든 줄을 하나씩 순회\n","    line = line.strip()  # 줄 끝의 줄바꿈 문자 등을 제거하여 깔끔한 문자열로 만듦\n","    if len(line) == 0:  # 빈 줄인 경우는 건너뛰기\n","        continue  # 다음 줄로 넘어감\n","    image_and_caption = line.split(\"\\t\")  # 탭 문자 기준으로 이미지 정보와 문장을 분리\n","    if len(image_and_caption) != 2:  # 만약 탭으로 나눈 결과가 2개가 아니라면 형식이 이상한 것이므로\n","        continue  # 해당 줄은 건너뛰고 다음 줄로 이동\n","    image_id_raw, caption_raw = image_and_caption  # 왼쪽은 이미지+번호, 오른쪽은 문장 부분으로 변수에 저장\n","    image_filename = image_id_raw.split(\"#\")[0]  # '파일이름#번호' 형태에서 앞부분(파일 이름)만 사용\n","    cleaned = clean_sentence(caption_raw)  # 위에서 정의한 함수로 문장을 전처리\n","    if len(cleaned.split()) < 3:  # 단어 수가 너무 적은 문장은 학습에 별 도움이 안 되므로 제외\n","        continue  # 다음 줄로 넘어감\n","    captions_dict.setdefault(image_filename, []).append(cleaned)  # 해당 이미지 파일 이름에 문장 추가\n","\n","print(\"이미지 개수(캡션 포함):\", len(captions_dict))  # 캡션이 있는 이미지가 몇 개인지 출력\n","\n","# 한 이미지에 어떤 캡션들이 들어 있는지 예시로 하나만 출력\n","sample_key = next(iter(captions_dict.keys()))  # 딕셔너리에서 임의의 첫 번째 key를 가져옴\n","print(\"예시 이미지 파일 이름:\", sample_key)  # 선택된 이미지 파일 이름 출력\n","print(\"해당 이미지의 캡션들:\")  # 그 이미지에 대응되는 문장들을 출력하겠다는 안내 메시지\n","for c in captions_dict[sample_key]:  # 선택된 이미지에 대한 캡션 리스트를 순회\n","    print(\"-\", c)  # 각 캡션을 한 줄에 하나씩 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XWIzant6zU8","executionInfo":{"status":"ok","timestamp":1764666651250,"user_tz":-540,"elapsed":267,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"8be7fcd0-6ddd-42e3-8d07-cf424ee2964c"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["40460\n","이미지 개수(캡션 포함): 8092\n","예시 이미지 파일 이름: 1000268201_693b08cb0e.jpg\n","해당 이미지의 캡션들:\n","- a child in a pink dress is climbing up a set of stairs in an entry way\n","- a girl going into a wooden building\n","- a little girl climbing into a wooden playhouse\n","- a little girl climbing the stairs to her playhouse\n","- a little girl in a pink dress going into a wooden cabin\n"]}]},{"cell_type":"code","source":["# 작은 서브셋 사용하기\n","\n","all_image_filenames = list(captions_dict.keys())\n","\n","subset_size = 200\n","if len(all_image_filenames) < subset_size:\n","    subset_size = len(all_image_filenames)\n","\n","small_image_filenames = random.sample(all_image_filenames, subset_size)\n","len(small_image_filenames)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQYZw74D8rZ-","executionInfo":{"status":"ok","timestamp":1764666651276,"user_tz":-540,"elapsed":12,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"277f4978-788d-46e8-d5f6-9e3b57924d60"},"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["200"]},"metadata":{},"execution_count":111}]},{"cell_type":"markdown","source":["단어 사전(vocabulary) 만들기\n","- 이미지 캡셔닝에서 문장을 다룰려면 단어 >> 숫자(index) 바꿔야 함\n","- 특별토큰(special token)\n","  - pad , start, end, unk"],"metadata":{"id":"Idn_cOx7CcN8"}},{"cell_type":"code","source":["# ===== 7. 단어 사전 구성 =====\n","special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]  # 특별한 의미를 가지는 4개의 특수 토큰 리스트\n","\n","word_counter = Counter()  # 각 단어가 몇 번 등장했는지 세기 위한 Counter 객체\n","for img in small_image_filenames:  # 선택된 서브셋 이미지들에 대해서만 반복\n","    for cap in captions_dict[img]:  # 각 이미지에 대해 여러 캡션들을 순회\n","        for w in cap.split():  # 문장을 공백 기준으로 나누어 단어 리스트를 얻음\n","            word_counter[w] += 1  # 해당 단어의 등장 빈도를 1 증가시킴\n","\n","min_freq = 3  # 단어가 최소 몇 번 이상 나타나야 사전에 포함할지 기준 (여기서는 3번 이상)\n","vocab_words = [w for w, c in word_counter.items() if c >= min_freq]  # 등장 빈도가 기준 이상인 단어만 추려서 리스트 생성\n","print(\"기준 이상으로 등장한 단어 수:\", len(vocab_words))  # 사전에 포함될 일반 단어 수를 출력\n","\n","idx2word = []  # 인덱스에서 단어로 바꾸기 위한 리스트(인덱스 -> 단어)\n","idx2word.extend(special_tokens)  # 앞쪽에 특수 토큰들을 순서대로 추가\n","idx2word.extend(sorted(vocab_words))  # 나머지 단어들을 정렬하여 뒤에 붙임\n","\n","word2idx = {w: i for i, w in enumerate(idx2word)}  # 단어에서 인덱스로 바꾸기 위한 딕셔너리(단어 -> 인덱스)\n","\n","pad_idx = word2idx[\"<pad>\"]  # 패딩 토큰의 인덱스를 변수로 저장 (나중에 손실 계산에서 무시할 때 사용)\n","start_idx = word2idx[\"<start>\"]  # 문장 시작 토큰의 인덱스\n","end_idx = word2idx[\"<end>\"]  # 문장 끝 토큰의 인덱스\n","unk_idx = word2idx[\"<unk>\"]  # 사전에 없는 단어를 대신할 토큰의 인덱스\n","\n","vocab_size = len(idx2word)  # 최종 단어 사전의 크기(특수 토큰 포함)\n","print(\"최종 단어 사전 크기:\", vocab_size)  # 사전 크기를 출력\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KygT6qv_BLif","executionInfo":{"status":"ok","timestamp":1764666651287,"user_tz":-540,"elapsed":9,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"e7945fd2-7b73-4d3d-9f42-2b85bfa193a6"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["기준 이상으로 등장한 단어 수: 482\n","최종 단어 사전 크기: 486\n"]}]},{"cell_type":"code","source":["# ===== 8. 문장을 숫자 시퀀스로 변환하는 함수 =====\n","def sentence_to_indices(sentence: str, max_len: int = 20):  # 문장과 최대 길이를 받아서 인덱스 리스트로 변환하는 함수\n","    tokens = sentence.split()  # 공백 기준으로 단어들을 분리하여 리스트로 만듦\n","    indices = [start_idx]  # 문장 시작을 의미하는 토큰 인덱스를 맨 앞에 추가\n","    for w in tokens:  # 문장의 각 단어에 대해 반복\n","        idx = word2idx.get(w, unk_idx)  # 단어가 사전에 있으면 그 인덱스를, 없으면 <unk> 인덱스를 가져옴\n","        indices.append(idx)  # 인덱스 리스트에 추가\n","        if len(indices) >= max_len - 1:  # 이미 충분히 길어졌다면 (마지막에 <end>를 하나 더 붙일 예정)\n","            break  # 더 이상 단어를 추가하지 않고 반복 종료\n","    indices.append(end_idx)  # 문장 끝을 의미하는 <end> 토큰 인덱스를 마지막에 추가\n","    # 길이가 너무 짧으면 뒤쪽을 <pad> 인덱스로 채워서 길이를 맞춤\n","    if len(indices) < max_len:  # 현재 길이가 최대 길이보다 짧다면\n","        indices.extend([pad_idx] * (max_len - len(indices)))  # 남은 부분을 모두 <pad>로 채움\n","    return indices  # 완성된 인덱스 리스트를 반환\n","\n","# 예시로 한 문장을 숫자 시퀀스로 변환해 보기\n","example_sentence = captions_dict[small_image_filenames[0]][0]  # 서브셋의 첫 번째 이미지에 대한 첫 번째 캡션 문장을 가져옴\n","print(\"예시 원본 문장:\", example_sentence)  # 원본 문장을 출력\n","example_indices = sentence_to_indices(example_sentence, max_len=10)  # 최대 길이를 10으로 제한하여 인덱스 시퀀스로 변환\n","print(\"숫자 시퀀스:\", example_indices)  # 변환된 인덱스 리스트를 출력\n","print(\"다시 단어로:\", [idx2word[i] for i in example_indices])  # 인덱스를 다시 단어로 바꿔서 사람이 읽을 수 있게 출력\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FloZpUDzCg9R","executionInfo":{"status":"ok","timestamp":1764666651313,"user_tz":-540,"elapsed":10,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"db50111a-0325-4b53-b392-62361a333fa9"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["예시 원본 문장: a man in a yellow shirt training a black poodle\n","숫자 시퀀스: [1, 4, 228, 186, 4, 484, 344, 3, 4, 2]\n","다시 단어로: ['<start>', 'a', 'man', 'in', 'a', 'yellow', 'shirt', '<unk>', 'a', '<end>']\n"]}]},{"cell_type":"markdown","source":["## 8. PyTorch Dataset 만들기 (이미지 + 캡션)\n","\n","딥러닝 학습을 위해서는 데이터를 **(입력, 정답)** 형태로 계속 공급해 주어야 합니다.\n","\n","- 입력(Input): 전처리된 이미지 텐서\n","- 정답(Target): 같은 이미지에 대한 캡션 문장(숫자 시퀀스)\n","\n","PyTorch의 `Dataset` 클래스를 상속하여, 우리가 원하는 형식으로 데이터를 꺼낼 수 있도록 만들어 봅니다."],"metadata":{"id":"hOe31k_jFkpI"}},{"cell_type":"code","source":["# ===== 9. PyTorch Dataset 정의 =====\n","class FlickrDataset(Dataset):  # PyTorch의 Dataset을 상속하여 FlickrDataset이라는 새로운 클래스 정의\n","    def __init__(self, image_folder, image_filenames, captions_dict, transform=None, max_len: int = 20):  # 생성자 정의\n","        self.image_folder = image_folder  # 이미지 파일이 저장된 폴더 경로를 멤버 변수로 저장\n","        self.image_filenames = image_filenames  # 사용할 이미지 파일 이름 리스트를 멤버 변수로 저장\n","        self.captions_dict = captions_dict  # 이미지별 캡션 정보를 담고 있는 딕셔너리를 멤버 변수로 저장\n","        self.transform = transform  # 이미지에 적용할 전처리(transform)를 멤버 변수로 저장\n","        self.max_len = max_len  # 캡션 최대 길이를 멤버 변수로 저장\n","\n","        # (이미지, 캡션) 쌍을 미리 펼쳐서 리스트로 저장\n","        self.samples = []  # 모든 (이미지 파일 이름, 캡션 문자열)을 저장할 리스트\n","        for img in self.image_filenames:  # 선택된 이미지 파일 이름들을 하나씩 순회\n","            for cap in self.captions_dict[img]:  # 각 이미지에 대해 여러 개의 캡션을 순회\n","                self.samples.append((img, cap))  # (이미지 파일 이름, 캡션 문자열) 튜플을 samples 리스트에 추가\n","\n","    def __len__(self):  # 데이터셋의 길이를 반환하는 메서드(필수 구현)\n","        return len(self.samples)  # (이미지, 캡션) 쌍의 총 개수를 반환\n","\n","    def __getitem__(self, idx):  # 인덱스로 데이터 하나를 꺼내는 메서드(필수 구현)\n","        img_name, caption = self.samples[idx]  # samples 리스트에서 idx 위치의 (이미지 파일 이름, 캡션)을 가져옴\n","        img_path = os.path.join(self.image_folder, img_name)  # 이미지 파일이 실제로 있는 전체 경로를 만듦\n","\n","        image = Image.open(img_path).convert(\"RGB\")  # 이미지 파일을 열고, RGB 3채널 이미지로 변환\n","        if self.transform is not None:  # 만약 transform이 정의되어 있다면\n","            image = self.transform(image)  # 이미지에 전처리(transform)를 적용\n","\n","        caption_indices = sentence_to_indices(caption, max_len=self.max_len)  # 문자열 캡션을 숫자 인덱스 시퀀스로 변환\n","        caption_tensor = torch.tensor(caption_indices, dtype=torch.long)  # 리스트를 PyTorch LongTensor로 변환\n","\n","        return image, caption_tensor  # (이미지 텐서, 캡션 텐서)를 튜플로 반환\n"],"metadata":{"id":"a8Ece7t-Cg_4","executionInfo":{"status":"ok","timestamp":1764666651317,"user_tz":-540,"elapsed":2,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["# ===== 10. 이미지 전처리(transform)와 DataLoader 정의 =====\n","image_transform = transforms.Compose([  # 여러 전처리 과정을 순서대로 적용하기 위한 Compose 사용\n","    transforms.Resize((224, 224)),  # 이미지를 224x224 크기로 리사이즈 (ResNet 입력 크기에 맞춤)\n","    transforms.ToTensor(),  # 이미지를 [0,1] 범위의 PyTorch 텐서(채널, 높이, 너비)로 변환\n","    transforms.Normalize(  # 이미지의 픽셀 값을 평균 0, 표준편차 1 근처로 맞추기 위한 정규화\n","        mean=[0.485, 0.456, 0.406],  # ImageNet 데이터셋에서 계산된 채널별 평균값\n","        std=[0.229, 0.224, 0.225],  # ImageNet 데이터셋에서 계산된 채널별 표준편차\n","    ),\n","])\n","\n","max_caption_len = 20  # 캡션의 최대 길이를 20 단어로 제한\n","\n","dataset = FlickrDataset(  # 위에서 정의한 FlickrDataset 클래스를 사용해 데이터셋 인스턴스를 생성\n","    image_folder=images_folder,  # 이미지가 저장된 폴더 경로 전달\n","    image_filenames=small_image_filenames,  # 사용할 이미지 파일 이름 리스트 전달\n","    captions_dict=captions_dict,  # 이미지별 캡션 딕셔너리 전달\n","    transform=image_transform,  # 이미지 전처리(transform) 전달\n","    max_len=max_caption_len,  # 캡션 최대 길이 전달\n",")\n","\n","print(\"(이미지, 캡션) 샘플 수:\", len(dataset))  # 데이터셋에 몇 개의 (이미지, 캡션) 쌍이 있는지 출력\n","\n","batch_size = 16  # 한 번에 모델에 넣을 데이터 개수(배치 크기)를 16으로 설정\n","\n","dataloader = DataLoader(  # PyTorch DataLoader를 사용하여 배치 단위로 데이터를 꺼낼 수 있도록 준비\n","    dataset,\n","    batch_size=batch_size,  # 위에서 설정한 배치 크기 사용\n","    shuffle=True,  # 매 epoch마다 데이터 순서를 섞어서 학습이 편향되지 않도록 설정\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UV8OHhDWChB3","executionInfo":{"status":"ok","timestamp":1764666651330,"user_tz":-540,"elapsed":11,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"03dfa65d-a9e7-41ef-dd05-3c8f3383e5cd"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["(이미지, 캡션) 샘플 수: 999\n"]}]},{"cell_type":"markdown","source":["CNN 인코더 정의\n","- 이미지 입력 받아 >> 특징(feauture) 그 이미지의 특징을 요약한 벡터 (feature vector) 만들어줘요\n","\n","- 사전학습된 resnet18 model 이용, 마지막 분류기(clf) 레이어 부분 제거 >> 512차원 특징벡터 사용"],"metadata":{"id":"MUNlSm-9Gwln"}},{"cell_type":"code","source":["# ===== 11. CNN 인코더 정의 (ResNet-18) =====\n","class EncoderCNN(nn.Module):  # PyTorch의 nn.Module을 상속하여 이미지 인코더 클래스를 정의\n","    def __init__(self, embed_size: int = 256):  # 임베딩 차원(embed_size)을 인자로 받아 초기화\n","        super().__init__()  # 부모 클래스(nn.Module)의 초기화 메서드 호출\n","        weights = ResNet18_Weights.DEFAULT  # torchvision에서 제공하는 ResNet-18의 기본 사전 학습 가중치 설정\n","        resnet = resnet18(weights=weights)  # 사전 학습된 가중치를 가진 ResNet-18 모델 불러오기\n","        modules = list(resnet.children())[:-1]  # 마지막 분류용 FC 레이어를 제외한 나머지 레이어들만 리스트로 추출\n","        self.cnn = nn.Sequential(*modules)  # 추출한 레이어들을 nn.Sequential로 묶어서 하나의 모듈로 구성\n","        self.fc = nn.Linear(resnet.fc.in_features, embed_size)  # ResNet 마지막 특성 차원에서 embed_size로 줄이는 선형 레이어\n","        self.bn = nn.BatchNorm1d(embed_size)  # 학습 안정화를 위해 배치 정규화 레이어 추가\n","\n","        for param in self.cnn.parameters():  # 사전 학습된 CNN 가중치들에 대해 반복\n","            param.requires_grad = False  # 입문용 예제에서는 CNN 부분은 학습하지 않고 고정(freeze)하여 빠르게 학습\n","\n","    def forward(self, images):  # 순전파(forward) 메서드 정의, 입력은 이미지 텐서\n","        features = self.cnn(images)  # CNN을 통과시켜 (배치, 채널, 1, 1) 형태의 특징 맵을 얻음\n","        features = features.view(features.size(0), -1)  # (배치, 채널, 1, 1)을 (배치, 채널) 형태로 펼침\n","        features = self.fc(features)  # 선형 레이어를 통과시켜 embed_size 차원의 벡터로 변환\n","        features = self.bn(features)  # 배치 정규화로 분포를 안정화\n","        return features  # 최종 이미지 특징 벡터를 반환\n"],"metadata":{"id":"DxgVfTHTHFff","executionInfo":{"status":"ok","timestamp":1764666651345,"user_tz":-540,"elapsed":4,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":["## 10. LSTM 디코더(Decoder) 정의\n","\n","디코더는 인코더가 만든 **이미지 특징 벡터**와 이전까지 생성된 단어들을 이용하여,\n","다음 단어를 하나씩 예측하는 **문장 생성기**입니다.\n","\n","1. 단어를 **임베딩(Embedding) 레이어**를 통해 숫자 벡터로 바꾼 뒤,\n","2. **LSTM** 에 순서대로 넣어 주고,\n","3. LSTM의 출력을 **Linear 레이어**를 통해 각 단어가 나올 확률로 변환합니다.\n"],"metadata":{"id":"QovhPlcKIjAp"}},{"cell_type":"code","source":["# ===== 12. LSTM 디코더 정의 =====\n","class DecoderRNN(nn.Module):  # PyTorch nn.Module을 상속하여 디코더 클래스를 정의\n","    def __init__(self, embed_size: int, hidden_size: int, vocab_size: int, num_layers: int = 1):  # 초기화 메서드\n","        super().__init__()  # 부모 클래스 초기화\n","        self.embed = nn.Embedding(vocab_size, embed_size)  # 단어 인덱스를 embed_size 차원의 벡터로 바꿔주는 임베딩 레이어\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)  # LSTM 레이어 정의 (입력: embed_size, 은닉: hidden_size)\n","        self.fc = nn.Linear(hidden_size, vocab_size)  # LSTM 출력을 단어 사전 크기만큼의 로짓(logit)으로 변환하는 선형 레이어\n","\n","    def forward(self, features, captions):  # 순전파 메서드, features: 이미지 벡터, captions: 정답 캡션 시퀀스\n","        embeddings = self.embed(captions)  # (배치, 시퀀스 길이) 형태의 캡션 인덱스를 임베딩 벡터로 변환\n","        features = features.unsqueeze(1)  # (배치, embed_size)를 (배치, 1, embed_size)로 차원 확장하여 LSTM 첫 입력으로 사용\n","        inputs = torch.cat((features, embeddings[:, :-1, :]), dim=1)  # 이미지 특징 뒤에 캡션의 마지막 토큰을 제외한 부분을 이어붙여 입력 시퀀스 생성\n","        outputs, _ = self.lstm(inputs)  # LSTM에 입력 시퀀스를 넣어 전체 시퀀스에 대한 은닉 상태 출력\n","        outputs = self.fc(outputs)  # 각 시점의 LSTM 출력을 단어 사전 크기의 로짓으로 변환\n","        return outputs  # (배치, 시퀀스 길이, vocab_size) 형태의 예측 결과 반환\n","\n","    def sample(self, features, max_len=20):  # 학습된 모델로부터 실제 문장을 생성하기 위한 메서드\n","        generated_indices = []  # 생성된 단어 인덱스를 순서대로 저장할 리스트\n","        inputs = features.unsqueeze(1)  # (배치=1, 1, embed_size) 형태로 LSTM 입력 준비\n","        states = None  # LSTM의 초기 은닉 상태와 셀 상태는 None으로 두면 자동 초기화\n","        for _ in range(max_len):  # 최대 max_len 길이만큼 단어를 생성\n","            outputs, states = self.lstm(inputs, states)  # 현재 입력과 상태를 LSTM에 넣어 한 시점의 출력을 얻음\n","            outputs = self.fc(outputs.squeeze(1))  # LSTM 출력을 선형 레이어에 통과시켜 단어별 로짓으로 변환\n","            _, predicted = outputs.max(1)  # 가장 확률이 높은 단어 인덱스를 선택\n","            generated_indices.append(predicted.item())  # 선택된 인덱스를 리스트에 추가\n","            if predicted.item() == end_idx:  # 만약 <end> 토큰이 나오면 문장 생성을 멈춤\n","                break  # 반복문 종료\n","            inputs = self.embed(predicted).unsqueeze(1)  # 예측된 단어를 임베딩하여 다음 시점의 입력으로 사용\n","        return generated_indices  # 생성된 단어 인덱스 리스트를 반환\n"],"metadata":{"id":"VMVNEE-6Ice2","executionInfo":{"status":"ok","timestamp":1764666651360,"user_tz":-540,"elapsed":3,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["# ===== 13. 모델 인스턴스 생성 및 학습 설정 =====\n","embed_size = 256  # 이미지 특징 벡터와 단어 임베딩 벡터의 차원을 256으로 설정\n","hidden_size = 512  # LSTM 은닉 상태의 차원을 512로 설정\n","\n","encoder = EncoderCNN(embed_size=embed_size).to(device)  # EncoderCNN 인스턴스를 만들고, GPU/CPU 디바이스로 이동\n","decoder = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size).to(device)  # DecoderRNN 인스턴스를 만들고 디바이스로 이동\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)  # 손실 함수로 다중 클래스 분류에 사용하는 CrossEntropyLoss를 사용, 패딩 토큰은 무시\n","params = list(decoder.parameters()) + [p for p in encoder.fc.parameters()] + [p for p in encoder.bn.parameters()]  # 학습할 파라미터들만 모아서 리스트로 생성\n","optimizer = torch.optim.Adam(params, lr=1e-3)  # Adam 옵티마이저를 사용하여 파라미터를 업데이트, 학습률은 0.001로 설정\n"],"metadata":{"id":"0jGIq0ymLGWQ","executionInfo":{"status":"ok","timestamp":1764666651562,"user_tz":-540,"elapsed":199,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","\n","num_epoch = 2\n","\n","# 학습 모드로 설정\n","encoder.train()\n","decoder.train()\n","\n","for epoch in range(num_epoch):\n","    total_loss = 0.0 # 한 epoch 동안 손실 누적\n","\n","    for images, captions in dataloader:\n","        images = images.to(device)\n","        captions = captions.to(device)\n","\n","        optimizer.zero_grad() # 이전 배치에서 누적된 기울기 모두 0으로 초기화\n","\n","        features = encoder(images) # 이미지 >> 인코더(cnn) >> 이미지 특징벡터\n","        outputs = decoder(features, captions)\n","        # (이미지특징, 캡션정답) >> 디코더에 넣어 단어별 예측 로짓 얻음.\n","\n","        # CrossEntropyLoss(배치*seq_len, vocab_size) 손실계산\n","        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n","\n","        loss.backward() # 역전파 수행\n","        optimizer.step() # 가중치 업데이트\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader) # 한 epoch 동안 평균 손실 계산\n","    print(f\"Epoch [{epoch+1}/{num_epoch}], 평균손실: {avg_loss: .4f} \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"8yzk0SvUMFPn","executionInfo":{"status":"error","timestamp":1764666655319,"user_tz":-540,"elapsed":45,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}},"outputId":"d9f7a1b0-da43-4867-b496-55c9233d0b40"},"execution_count":120,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'Flickr8k_Dataset/2064780645_8f28a1529f.jpg'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3554373052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;31m# 한 epoch 동안 손실 누적\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1255396774.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 이미지 파일이 실제로 있는 전체 경로를 만듦\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 이미지 파일을 열고, RGB 3채널 이미지로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 만약 transform이 정의되어 있다면\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 이미지에 전처리(transform)를 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Flickr8k_Dataset/2064780645_8f28a1529f.jpg'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ky2R_9ZgPCQG","executionInfo":{"status":"aborted","timestamp":1764666651722,"user_tz":-540,"elapsed":2,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"execution_count":null,"outputs":[]}]}