{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1764310474214,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "dl0b2qrvGGFo",
    "outputId": "53ac2e84-4ac3-4d8f-d81f-6ba37b770204"
   },
   "outputs": [],
   "source": [
    "# Python Imaging Library (PIL)의 Image 모듈을 불러옴. 이미지 파일을 열고 조작하는 데 사용함.\n",
    "from PIL import Image\n",
    "# Matplotlib의 pyplot 모듈을 plt 별칭으로 불러옴. 시각화에 사용함.\n",
    "import matplotlib.pyplot as plt\n",
    "# PyTorch 핵심 라이브러리를 불러옴.\n",
    "import torch\n",
    "# 신경망 레이어(nn) 모듈을 불러옴.\n",
    "import torch.nn as nn\n",
    "# 옵티마이저(optim) 모듈을 불러옴.\n",
    "import torch.optim as optim\n",
    "# TorchVision 라이브러리를 불러옴. (이미지 변환 및 데이터셋 등에 사용)\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습에 사용할 장치(Device)를 설정함. CUDA(GPU)가 사용 가능하면 'cuda'를, 아니면 'cpu'를 선택함.\n",
    "dvc = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqVgNZQPGVGu"
   },
   "outputs": [],
   "source": [
    "# pil -> tensor\n",
    "def image_to_tensor(image_filepath, image_dimension=128):\n",
    "    img = Image.open(image_filepath).convert('RGB') # BGR -> RGB\n",
    "    # 디버깅 : pil -> plt 출력\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    # 이미지 해상도별 조정\n",
    "    if max(img.size) <= image_dimension:\n",
    "        img_size = max(img.size)\n",
    "    else:\n",
    "        img_size = image_dimension\n",
    "    # tensor pipeline\n",
    "    torch_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(img_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    # 변환 적용\n",
    "    img = torch_transform(img).unsqueeze(0)\n",
    "\n",
    "    return img.to(dvc, torch.float)\n",
    "\n",
    "style_image = image_to_tensor('vangogh_starry_night.jpg')\n",
    "content_image = image_to_tensor('Tuebingen_Neckarfront.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(ip):\n",
    "    n_batch, n_channel, h, w = ip.size()\n",
    "    feats = ip.view(n_batch, n_channel, h * w)\n",
    "    gram_mat = torch.bmm(feats, feats.transpose(1,2))\n",
    "    return gram_mat.div(n_batch * n_channel * h * w) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 부분 확인 및 출력\n",
    "vgg19_model = torchvision.models.vgg19(pretrained=True).to(dvc)\n",
    "vgg19_model = vgg19_model.features\n",
    "print(vgg19_model)\n",
    "\n",
    "# freeze\n",
    "for p in vgg19_model.parameters():\n",
    "  p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_indices = []\n",
    "\n",
    "for i in range(len(vgg19_model)):\n",
    "    # MaxPool2d를 AvgPool2d로 변경\n",
    "    if vgg19_model[i]._get_name() == 'MaxPool2d':\n",
    "        vgg19_model[i]  = nn.AvgPool2d(kernel_size=vgg19_model[i].kernel_size,\n",
    "                                   stride = vgg19_model[i].stride,\n",
    "                                   padding=vgg19_model[i].padding)\n",
    "    if vgg19_model[i]._get_name() == 'Conv2d':\n",
    "        conv_indices.append(i)\n",
    "\n",
    "conv_indices = dict(enumerate(conv_indices, 1)) # {1:0, 2:2, 3:5, ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip until the last relevant layer\n",
    "layers = {1:'s', 2:'s', 3:'s', 4:'sc', 5:'s'}\n",
    "# vgg feature extractor -> nn.ModuleList\n",
    "# - list : 인덱싱, 슬라이싱 가능\n",
    "# - <> nn.Sequential 불가\n",
    "# Avgpool 설정 : 모듈 block화 -> 출력\n",
    "vgg_layers = nn.ModuleList(vgg19_model)\n",
    "\n",
    "# 디버깅: 가장 깊은 레이어 확인\n",
    "last_layer_idx = conv_indices[max(conv_indices.keys())]\n",
    "\n",
    "vgg_layers_trimmed = vgg_layers[:last_layer_idx+1] # vgg_layers 보존\n",
    "neural_style_transfer_model = nn.Sequential(*vgg_layers_trimmed) # 언패킹\n",
    "\n",
    "# * : 언패킹 연산자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_style_transfer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 가지 테스트 방향\n",
    "# 1. 최적화 대상 이미지를 콘텐츠 이미지와 동일하게 초기화\n",
    "# ip_image = content_image.clone()\n",
    "\n",
    "# 2 선택\n",
    "# 2. 최적화 대상(ip_image)를 콘텐츠 이미지 크기와 동일한 랜덤 노이즈로 초기화\n",
    "ip_image = torch.randn(content_image.size(), device=dvc)\n",
    "\n",
    "# 초기화된 노이즈 시각화\n",
    "plt.figure()\n",
    "# 텐서에서 배치차원(0번)제거, 기울기 비활성화, 넘파이로 변환, 채널 순서\n",
    "plt.imshow(ip_image.squeeze(0).cpu().detach().numpy().transpose(1,2,0).clip(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 300\n",
    "# 손실 가중치 설정 (스타일 손실을 콘텐츠 손실보다 훨씬 크게 설정)\n",
    "wt_style = 1e6 # GramMatrix 기반 -> 값의 크기가 작음\n",
    "wt_content = 1\n",
    "\n",
    "style_losses, content_losses = [], []\n",
    "opt = optim.Adam([ip_image.requires_grad_()], lr=0.1)\n",
    "\n",
    "for curr_epoch in range(1, n_epoch+1):\n",
    "    ip_image.data.clamp_(0, 1) # 생성된 이미지 자름\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # 누적 손실변수 초기화\n",
    "    epoch_style_loss, epoch_content_loss = 0, 0\n",
    "\n",
    "    for k in layers.keys():\n",
    "        if 'c' in layers[k]:\n",
    "            # 콘텐츠 이미지의 '특징추출' 진행, 변화계산 제외\n",
    "            target = neural_style_transfer_model[:conv_indices[k]+1](content_image).detach()\n",
    "            # 현재 이미지 특징추출\n",
    "            ip = neural_style_transfer_model[:conv_indices[k]+1](ip_image)\n",
    "            # 콘텐츠 손실 계산, 누적\n",
    "            epoch_content_loss += torch.nn.functional.mse_loss(ip, target)\n",
    "\n",
    "        if 's' in layers[k]:\n",
    "            # 스타일 이미지의 'gram matrix 계산' 진행, 변화계산 제외\n",
    "            target = gram_matrix(neural_style_transfer_model[:conv_indices[k]+1](style_image)).detach()\n",
    "            # 현재 이미지 특징추출\n",
    "            ip = gram_matrix(neural_style_transfer_model[:conv_indices[k]+1](ip_image))\n",
    "            # 스타일 손실 계산, 누적\n",
    "            epoch_style_loss += torch.nn.functional.mse_loss(ip, target)\n",
    "\n",
    "    # 누적 스타일 손실에 가중치 반영\n",
    "    epoch_style_loss *= wt_style\n",
    "    # 누적 컨텐츠 손실에 가중치 반영\n",
    "    epoch_content_loss *= wt_content\n",
    "    # 최종 손실 계산\n",
    "    total_loss = epoch_style_loss + epoch_content_loss\n",
    "    # 최종 손실 역전파\n",
    "    total_loss.backward()\n",
    "\n",
    "    # 50 에폭마다 현재 상태를 출력하고 시각화함.\n",
    "    if curr_epoch % 50 == 0:\n",
    "        print(f\"epoch number {curr_epoch}\")\n",
    "        print(f\"style loss = {epoch_style_loss}, content loss = {epoch_content_loss}\")\n",
    "        plt.figure()\n",
    "        plt.title(f\"epoch number {curr_epoch}\")\n",
    "        # 생성된 이미지를 역변환(clamp, squeeze, numpy, transpose)하여 시각화함.\n",
    "        plt.imshow(ip_image.data.clamp_(0, 1).squeeze(0).cpu().detach().numpy().transpose(1,2,0))\n",
    "        plt.show()\n",
    "        # 손실 값을 리스트에 저장함.\n",
    "        style_losses.append(epoch_style_loss.item())\n",
    "        content_losses.append(epoch_content_loss.item())\n",
    "\n",
    "    # 옵티마이저를 사용하여 변화도 방향으로 이미지 픽셀 값을 업데이트함.\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(50, 300+1, 50), style_losses, label='style_loss')\n",
    "plt.plot(range(50, 300+1, 50), content_losses, label='content_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDUPk0JAH865cHfn6TBGEP",
   "gpuType": "T4",
   "mount_file_id": "15VBWs5woEYGuS_9ZbFNVyuwSnAkdVT0L",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
