{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강의_6기_AI응용_6차시_01_GramMatrix과 Loss.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Style Transfer (스타일 전이)\n",
    "- target: 학습이 도달해야 할 기준 (reference 정답)  \n",
    "  - 생성되는 이미지(변하는 이미지) 따라가야 할 목표 상태(정답)\n",
    "\n",
    "- content 목표 / style 목표\n",
    "  - content 목표 : 이미지 속 형태나 구조 유지\n",
    "    - target 콘텐츠 이미지의 feature map\n",
    "  - style 목표: 이미지 가지고 있는 질감, 색감, 패턴 재현\n",
    "     - target 스타일 이미지의 Gram Matrix\n",
    "\n",
    "- 원본(style, content) 에서 추출된 features 정보\n",
    "  - 생성 이미지가 학습을 통해 그 값과 가까워지는 것\n",
    "  - loss가 0에 가까워 지는 지점 확보\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5694,
     "status": "ok",
     "timestamp": 1764305821181,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "uy7s3xvA-Hro"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1764305821186,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "8D8WfVJC_LBZ"
   },
   "outputs": [],
   "source": [
    "# Gram Matrix\n",
    "class GramMatrix(nn.Module):\n",
    "  def forward(self, input):\n",
    "    b,c,h,w = input.size()\n",
    "    F = input.view(b, c, h*w) # flatten \n",
    "\n",
    "    # bmm : 배치 행렬곱\n",
    "    # features: (b, c, h*w)\n",
    "    # features.transpose(1,2): (b, h*w, c)\n",
    "    # 출력 : (b, c, c) \n",
    "    G = torch.bmm(F, F.transpose(1,2))\n",
    "    G.div_(h*w) # 정규화, 학습 목적으로 수식 단순하게 표현\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1764305821190,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "8Xf30kNh_LEH"
   },
   "outputs": [],
   "source": [
    "# GramMSELoss 정의\n",
    "class GramMSELoss(nn.Module):\n",
    "  def forward(self, input, target):\n",
    "    out = nn.MSELoss()(GramMatrix()(input),target)\n",
    "    return (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvEMIjbzVriN"
   },
   "source": [
    "Content-style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1764305821232,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "iCp_Mnnw_LGb"
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, pool='max'):\n",
    "        super(VGG, self).__init__()\n",
    "        #vgg modules\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        if pool == 'max':     # 특징 최대값 추출(뚜렷한 특징)\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pool == 'avg':  # 특징 평균 추출(부드러운 특징)\n",
    "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, out_keys):\n",
    "        out = {}\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
    "        out['p3'] = self.pool3(out['r34'])\n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
    "        out['p4'] = self.pool4(out['r44'])\n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
    "        out['p5'] = self.pool5(out['r54'])\n",
    "        return [out[key] for key in out_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DG_YlCkmWPaf"
   },
   "source": [
    "[VGG 구조 패턴]\n",
    "\n",
    "- Block 1: 3→64→64 (얕은 특징: 선, 모서리)\n",
    "- Block 2: 64→128→128 (중간 특징: 질감)\n",
    "- Block 3: 128→256→256→256 (깊은 특징: 패턴)\n",
    "- Block 4: 256→512→512→512 (더 복잡한 특징)\n",
    "- Block 5: 512→512→512→512 (추상적 특징)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 4720,
     "status": "ok",
     "timestamp": 1764305825954,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "QkLT5gKNWVPK",
    "outputId": "12e74d59-cf4e-4801-d757-99b29dfcdc32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nopt_img = content_image.clone().detach().requires_grad_(True)\\n# content_image (데이터, 값) 복제해서 기존 그래프 분리(detach) >> 기울기 계산\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = VGG()\n",
    "\n",
    "img1 = \"vangogh_starry_night.jpg\"\n",
    "img2 = \"Tuebingen_Neckarfront.jpg\"\n",
    "\n",
    "img1 = Image.open(img1)\n",
    "img2 = Image.open(img2)\n",
    "imgs = []\n",
    "imgs.append(img1)\n",
    "imgs.append(img2)\n",
    "\n",
    "img_size = 512\n",
    "prep = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),  # Tensor (c h w) [0 - 1] \n",
    "            transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), # RGB >> BGR\n",
    "            transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961],\n",
    "                                 std=[1,1,1]), # ImageNet  평균값 정규화\n",
    "            transforms.Lambda(lambda x: x.mul_(255)),\n",
    "            # pixel [0,255] 스케일 >> 픽셀 크기를 VGG 입력 스케일에 맞춤\n",
    "        ])\n",
    "\n",
    "# PIL -> tensor\n",
    "# imgs 이미지 리스트 전체\n",
    "imgs_torch = [prep(img) for img in imgs]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in img_torch]\n",
    "    imgs_torch = [img.unsqueeze(0).cuda() for img in imgs_torch]\n",
    "else:\n",
    "    imgs_torch = [img.unsqueeze(0) for img in imgs_torch]\n",
    "\n",
    "style_image, content_image = imgs_torch\n",
    "\n",
    "# 형태 유지하며 스타일만 변형되도록 학습(SGD) 시작\n",
    "# 이미지 학습(이미지 개별픽셀 학습O) <> 이미지 특징학습X\n",
    "# 스타일 전이학습에서 CNN 가중치 학습X\n",
    "# content_image는 채점 기준표\n",
    "# opt_img = Variable(content_image.data.clone(), requires_grad=True)\n",
    "opt_img = content_image.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1764305892043,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "iXL7kg4GZCk3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vgg = vgg.cuda()\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 스타일 손실을 계산할 VGG 레이어 이름 정의\n",
    "# style_layers : VGG 내부의 특정 합성곱 레이어\n",
    "\n",
    "style_layers = ['r11','r21','r31','r41', 'r51']\n",
    "\n",
    "# 콘텐츠 손실을 계산할 VGG 레이어 이름 정의\n",
    "# (일반적으로 중간정도 레이어 중 하나 사용)\n",
    "content_layers = ['r42']\n",
    "\n",
    "loss_layers = style_layers + content_layers\n",
    "\n",
    "# 각 스타일에 대해 GramMSELoss 모듈을 사용하고, 콘텐츠에 대해 MSELoss 모듈을 사용하도록 리스트를 정의함.\n",
    "# GramMSELoss 모듈 : 스타일 손실(loss) 계산\n",
    "# [GramMSELoss, GramMSELoss, ..., nn.MSELoss] 형태가 됨.\n",
    "\n",
    "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # loss_fns 리스트의 요소들을 새로운 모듈 인스턴스로 만들고 .cuda()를 적용\n",
    "    # 리스트 복사를 방지하고 정확하게 GPU로 이동\n",
    "    loss_fns = [GramMSELoss().to(device) for _ in style_layers] + \\\n",
    "               [nn.MSELoss().to(device) for _ in content_layers]\n",
    "    #  for _ in style_layers : style layers 개수만큼 GramMSELoss() 넣어요 >> 인스턴스\n",
    "    #  >> 각각 새로 생성\n",
    "else:\n",
    "    # CPU 사용 시에도 동일한 로직으로 인스턴스화\n",
    "    loss_fns = [GramMSELoss().to(device) for _ in style_layers] + \\\n",
    "               [nn.MSELoss().to(device) for _ in content_layers]\n",
    "\n",
    "# 스타일 손실에 적용할 가중치(beta)를 정의\n",
    "# 깊은 레이어일수록(복잡한 패턴을 추출하는 레이어) 낮은 가중치를 주는 경향이 있음\n",
    "# 왜? 가중치가 감소되니깐\n",
    "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "# [64,128,256,512,512] : channel(c) 수\n",
    "# 1e3/n**2 : 보정 값. 깊은 레이어일 수로 feature 수가 많고 값도 커지므로 스타일 손실이 너무 커지는 거 방지\n",
    "\n",
    "# 콘텐츠 손실에 부여할 가중치(alpha)를 정의\n",
    "content_weights = [1e0]\n",
    "# 1e0 : 1\n",
    "\n",
    "weights = style_weights + content_weights\n",
    "\n",
    "# 최적화 목표값 (style targets) 계산\n",
    "# style_image >> VGG 통과 >> 각 스타일 레이어 Gram Matrix 계산\n",
    "# >> 변화도 추적에서 제외 (detach)\n",
    "style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
    "# vgg(style_image, style_layers) 지정한 레이어들의 특징맵(feature map )리스트로 반환\n",
    "# [A_r11, A_r21.....] 여기서 A.shape (b,c,h,w)\n",
    "# GramMatrix(A) >> 형태 변환 (b, c, c) >> 스타일 표현\n",
    "# .detach() 계산 그래프 분리(역전파시 gradient 계산되지 않도록)\n",
    "# >> 왜? style_targets 고정된 값(ground truth)\n",
    "# style_targets? 각 스타일 레이어에 대한 스타일 이미지 Gram Matrix 목록\n",
    "\n",
    "# 최적화 목표값(content targets)을 계산함.\n",
    "# 콘텐츠 이미지(content_image)를 VGG에 통과시켜 콘텐츠 레이어의 특징 맵을 추출하고 변화도 추적에서 제외(detach)했음.\n",
    "# content_image 또한 이미 .cuda() 또는 .to(device)로 GPU에 로드되어 있다고 가정합니다.\n",
    "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
    "\n",
    "# 최종적으로 사용할 모든 목표값 리스트를 정의함.\n",
    "targets = style_targets + content_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1764306114334,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "G0Tjz08czpRi",
    "outputId": "174c4217-d7c1-49bb-a66a-710403600793"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[317.9097,  14.2340,   9.5391,  ...,  30.3477, 125.0743,  30.5910],\n",
       "          [ 14.2340, 539.9493,  55.6673,  ...,  72.0618,  10.4826, 608.8537],\n",
       "          [  9.5391,  55.6673,  41.4998,  ...,  77.6339,   4.3284,  68.2613],\n",
       "          ...,\n",
       "          [ 30.3477,  72.0618,  77.6339,  ..., 575.5036,  22.8726,  35.3250],\n",
       "          [125.0743,  10.4826,   4.3284,  ...,  22.8726,  65.3410,   9.6822],\n",
       "          [ 30.5910, 608.8537,  68.2613,  ...,  35.3250,   9.6822, 791.8442]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[3.2912e+00, 1.6034e-02, 4.9145e+00,  ..., 7.2559e-03,\n",
       "           7.6530e-04, 8.2010e-01],\n",
       "          [1.6034e-02, 2.4941e-01, 2.2831e-01,  ..., 1.6959e-02,\n",
       "           4.3480e-05, 4.6047e-01],\n",
       "          [4.9145e+00, 2.2831e-01, 1.4602e+01,  ..., 7.5723e-01,\n",
       "           6.7991e-03, 4.6019e+00],\n",
       "          ...,\n",
       "          [7.2559e-03, 1.6959e-02, 7.5723e-01,  ..., 5.8756e-01,\n",
       "           4.1775e-04, 9.8683e-01],\n",
       "          [7.6530e-04, 4.3480e-05, 6.7991e-03,  ..., 4.1775e-04,\n",
       "           1.5398e-03, 1.8569e-04],\n",
       "          [8.2010e-01, 4.6047e-01, 4.6019e+00,  ..., 9.8683e-01,\n",
       "           1.8569e-04, 1.0112e+01]]], device='cuda:0'),\n",
       " tensor([[[6.2932e-01, 9.8548e-02, 2.4599e-02,  ..., 2.1865e-05,\n",
       "           2.3033e-01, 1.1851e-01],\n",
       "          [9.8548e-02, 2.0741e-01, 3.6110e-03,  ..., 1.7660e-06,\n",
       "           3.1466e-01, 1.1931e-02],\n",
       "          [2.4599e-02, 3.6110e-03, 1.0786e-02,  ..., 0.0000e+00,\n",
       "           1.3253e-02, 8.6823e-03],\n",
       "          ...,\n",
       "          [2.1865e-05, 1.7660e-06, 0.0000e+00,  ..., 4.5854e-06,\n",
       "           5.9828e-05, 0.0000e+00],\n",
       "          [2.3033e-01, 3.1466e-01, 1.3253e-02,  ..., 5.9828e-05,\n",
       "           7.1620e-01, 5.3537e-02],\n",
       "          [1.1851e-01, 1.1931e-02, 8.6823e-03,  ..., 0.0000e+00,\n",
       "           5.3537e-02, 8.4082e-02]]], device='cuda:0'),\n",
       " tensor([[[1.1721e-05, 9.6406e-06, 1.5839e-07,  ..., 0.0000e+00,\n",
       "           4.3275e-06, 1.1808e-05],\n",
       "          [9.6406e-06, 5.2657e-04, 1.7026e-06,  ..., 0.0000e+00,\n",
       "           4.5520e-04, 2.0379e-04],\n",
       "          [1.5839e-07, 1.7026e-06, 5.1036e-07,  ..., 0.0000e+00,\n",
       "           1.3644e-06, 1.0716e-06],\n",
       "          ...,\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [4.3275e-06, 4.5520e-04, 1.3644e-06,  ..., 0.0000e+00,\n",
       "           1.2747e-03, 2.6950e-04],\n",
       "          [1.1808e-05, 2.0379e-04, 1.0716e-06,  ..., 0.0000e+00,\n",
       "           2.6950e-04, 1.8795e-04]]], device='cuda:0'),\n",
       " tensor([[[2.5108e-07, 0.0000e+00, 0.0000e+00,  ..., 1.3633e-06,\n",
       "           8.9482e-07, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 9.1732e-08,  ..., 1.7608e-06,\n",
       "           8.6731e-07, 0.0000e+00],\n",
       "          ...,\n",
       "          [1.3633e-06, 0.0000e+00, 1.7608e-06,  ..., 2.5173e-04,\n",
       "           1.0966e-04, 0.0000e+00],\n",
       "          [8.9482e-07, 0.0000e+00, 8.6731e-07,  ..., 1.0966e-04,\n",
       "           4.8888e-05, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]]], device='cuda:0'),\n",
       " tensor([[[[2.8033e-02, 3.0695e-02, 3.1722e-02,  ..., 2.4232e-02,\n",
       "            2.1942e-02, 1.5214e-02],\n",
       "           [3.1675e-02, 3.5412e-02, 3.3735e-02,  ..., 2.4780e-02,\n",
       "            2.5902e-02, 1.5491e-02],\n",
       "           [4.5547e-02, 4.2933e-02, 3.9397e-02,  ..., 3.1027e-02,\n",
       "            2.9949e-02, 1.3394e-02],\n",
       "           ...,\n",
       "           [6.3273e-02, 5.6069e-02, 4.6420e-02,  ..., 2.9790e-02,\n",
       "            3.3365e-02, 1.6391e-02],\n",
       "           [5.6744e-02, 5.6271e-02, 4.6523e-02,  ..., 3.3239e-02,\n",
       "            3.3087e-02, 1.4694e-02],\n",
       "           [4.7421e-02, 4.7198e-02, 4.0874e-02,  ..., 3.2783e-02,\n",
       "            3.2800e-02, 1.9596e-02]],\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.7031e-02, 2.3644e-02, 2.7832e-02,  ..., 2.6612e-02,\n",
       "            2.6927e-02, 3.1778e-02],\n",
       "           [1.1483e-02, 2.6818e-02, 3.2296e-02,  ..., 3.1957e-02,\n",
       "            3.1098e-02, 3.6927e-02],\n",
       "           [9.5704e-03, 1.8807e-02, 2.4788e-02,  ..., 2.9848e-02,\n",
       "            3.0735e-02, 3.5461e-02],\n",
       "           ...,\n",
       "           [1.2866e-02, 1.9047e-02, 3.1461e-02,  ..., 3.0532e-02,\n",
       "            2.8236e-02, 2.7705e-02],\n",
       "           [1.7715e-03, 9.8798e-03, 2.2104e-02,  ..., 2.5118e-02,\n",
       "            2.4186e-02, 2.6084e-02],\n",
       "           [2.4021e-05, 3.7345e-03, 1.1363e-02,  ..., 1.9383e-02,\n",
       "            1.6248e-02, 1.6561e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.5598e-02, 1.9915e-02, 2.1610e-02,  ..., 2.1861e-02,\n",
       "            1.7229e-02, 1.2985e-02],\n",
       "           [1.8068e-02, 2.6376e-02, 2.7728e-02,  ..., 2.9565e-02,\n",
       "            2.0817e-02, 1.0530e-02],\n",
       "           [2.3607e-02, 2.9557e-02, 2.8262e-02,  ..., 2.9850e-02,\n",
       "            1.8066e-02, 7.4638e-03],\n",
       "           ...,\n",
       "           [2.4667e-02, 2.7776e-02, 2.7813e-02,  ..., 2.4924e-02,\n",
       "            1.6989e-02, 6.2949e-03],\n",
       "           [3.1213e-02, 3.6215e-02, 3.5231e-02,  ..., 2.2677e-02,\n",
       "            1.1910e-02, 5.3243e-03],\n",
       "           [1.9680e-02, 1.9364e-02, 1.6277e-02,  ..., 1.2059e-02,\n",
       "            7.6956e-03, 0.0000e+00]],\n",
       " \n",
       "          [[1.3043e-02, 1.3201e-02, 1.2558e-02,  ..., 1.2416e-02,\n",
       "            1.8560e-02, 1.9820e-02],\n",
       "           [1.3486e-02, 7.0547e-03, 5.7812e-03,  ..., 7.4819e-04,\n",
       "            8.1046e-03, 1.6235e-02],\n",
       "           [1.0144e-02, 7.5980e-03, 1.0094e-02,  ..., 4.8696e-03,\n",
       "            1.2140e-02, 2.0828e-02],\n",
       "           ...,\n",
       "           [1.2507e-02, 6.1599e-03, 1.4144e-02,  ..., 1.2221e-02,\n",
       "            1.9710e-02, 2.3069e-02],\n",
       "           [1.1757e-02, 2.7998e-03, 7.6131e-03,  ..., 1.0416e-02,\n",
       "            1.2929e-02, 2.2959e-02],\n",
       "           [1.3215e-02, 1.3301e-02, 1.9588e-02,  ..., 1.6794e-02,\n",
       "            1.8510e-02, 2.1335e-02]],\n",
       " \n",
       "          [[7.0083e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3268e-04, 1.9136e-03, 2.2313e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.4037e-03],\n",
       "           [2.1342e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7582e-03, 5.2463e-03, 7.2561e-03,  ..., 5.9424e-03,\n",
       "            9.8216e-03, 9.3554e-03]]]], device='cuda:0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1764306265335,
     "user": {
      "displayName": "주식회사한시경",
      "userId": "01036671795204052858"
     },
     "user_tz": -540
    },
    "id": "q-IgOjhH1tSs",
    "outputId": "45b841c7-0a75-4538-dea0-dbd647fa68cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  15.0610,   19.0610,   32.0610,  ...,   68.0610,   71.0610,\n",
       "             70.0610],\n",
       "          [  10.0610,   16.0610,   16.0610,  ...,   69.0610,   70.0610,\n",
       "             69.0610],\n",
       "          [   6.0610,   15.0610,   23.0610,  ...,   70.0610,   71.0610,\n",
       "             70.0610],\n",
       "          ...,\n",
       "          [ -81.9390,  -83.9390,  -84.9390,  ...,    5.0610,   -2.9390,\n",
       "             -0.9390],\n",
       "          [ -82.9390,  -85.9390,  -85.9390,  ...,   -8.9390,   -2.9390,\n",
       "             -5.9390],\n",
       "          [ -83.9390,  -88.9390,  -87.9390,  ...,   -7.9390,   -6.9390,\n",
       "            -15.9390]],\n",
       "\n",
       "         [[ -33.7790,  -32.7790,  -21.7790,  ...,   -2.7790,    0.2210,\n",
       "             -0.7790],\n",
       "          [ -33.7790,  -30.7790,  -34.7790,  ...,   -1.7790,   -0.7790,\n",
       "             -1.7790],\n",
       "          [ -31.7790,  -26.7790,  -21.7790,  ...,   -0.7790,    0.2210,\n",
       "             -0.7790],\n",
       "          ...,\n",
       "          [ -91.7790,  -93.7790,  -94.7790,  ...,   19.2210,   11.2210,\n",
       "             14.2210],\n",
       "          [ -92.7790,  -95.7790,  -95.7790,  ...,    5.2210,   11.2210,\n",
       "              9.2210],\n",
       "          [ -93.7790,  -98.7790,  -97.7790,  ...,    5.2210,    6.2210,\n",
       "             -1.7790]],\n",
       "\n",
       "         [[ -69.6800,  -70.6800,  -64.6800,  ...,  -61.6800,  -58.6800,\n",
       "            -59.6800],\n",
       "          [ -70.6800,  -70.6800,  -77.6800,  ...,  -60.6800,  -59.6800,\n",
       "            -60.6800],\n",
       "          [ -70.6800,  -66.6800,  -63.6800,  ...,  -59.6800,  -58.6800,\n",
       "            -59.6800],\n",
       "          ...,\n",
       "          [-100.6800, -102.6800, -103.6800,  ...,   26.3200,   24.3200,\n",
       "             31.3200],\n",
       "          [-101.6800, -104.6800, -104.6800,  ...,   10.3200,   21.3200,\n",
       "             22.3200],\n",
       "          [-102.6800, -107.6800, -106.6800,  ...,   10.3200,   16.3200,\n",
       "             11.3200]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = content_image.clone().requires_grad_(True)\n",
    "# input_image 는 VGG에 입력될 초기 이미지, 콘텐츠 이미지와 동일해야 함\n",
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNO7EDSD2SPe"
   },
   "outputs": [],
   "source": [
    "# L-BFGS 옵티마이저: 최적화 단계에서 closure 함수 요구\n",
    "optimizer = torch.optim.LBFGS([input_image], max_iter=1)\n",
    "# LBFGS([초기 이미지]) : CNN 가중치가 아니라 이미지 픽셀 학습\n",
    "# LBFGS (Quasi Newton optimizer) : 고차원 연속계산 적합(고품질 결과물)\n",
    "\n",
    "# optimizer = torch.optim.Adam([input_image], lr=0.01)\n",
    "# Adam 사용 가능 : 일반적으로 안정되면서 빠른 테스트 결과\n",
    "\n",
    "# 최적화 함수 세기 위한 변수\n",
    "n_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rS2yhLWz3MKA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Total Loss = 8689.9297\n",
      "Iteration 50: Total Loss = 0.0946\n",
      "Iteration 100: Total Loss = 0.0051\n",
      "Iteration 150: Total Loss = 0.0020\n",
      "Iteration 200: Total Loss = 0.0018\n",
      "Iteration 250: Total Loss = 0.0018\n",
      "Iteration 300: Total Loss = 0.0018\n",
      "Iteration 350: Total Loss = 0.0018\n",
      "Iteration 400: Total Loss = 0.0018\n",
      "Iteration 450: Total Loss = 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [0., 0., 0.,  ..., 1., 1., 1.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def closure():\n",
    "    global n_iter\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 생성된 이미지(input_image)를 VGG에 통과시켜 특징 맵을 추출합니다.\n",
    "    # vgg는 이전에 정의되어 GPU로 이동되었다고 가정합니다.\n",
    "    # input_image >> feature map 목록 [A_r11, A_r21, ...]\n",
    "    out = vgg(input_image, loss_layers)\n",
    "\n",
    "    # 총 손실을 계산합니다.\n",
    "    layer_losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    # 각 레이어별 손실(콘텐츠 손실, 스타일 손실) 계산\n",
    "    # 가중치 곱\n",
    "    # 전체 손실에 더함\n",
    "    for i, weight in enumerate(weights):\n",
    "        target = targets[i]     # 스타일 이미지 / 콘텐츠 이미지 타겟\n",
    "        feature = out[i]        # 현재 생성 이미지의 feature map\n",
    "        loss_fn = loss_fns[i]   # 해당 레이어(GramMSELoss, MSELoss)\n",
    "        # GramMatrix 계산이 필요한 스타일 레이어 처리 (GramMSELoss가 GramMatrix를 내부에서 처리한다고 가정)\n",
    "        # GramMatrix()가 별도 모듈이면, GramMSELoss 내부에 GramMatrix가 포함되어 있어야 합니다.\n",
    "\n",
    "        loss = weight * loss_fn(feature, target)\n",
    "        layer_losses.append(loss.item())\n",
    "        total_loss += loss\n",
    "\n",
    "    total_loss.backward() # input 이미지에 대해 계산\n",
    "\n",
    "    if n_iter % 50 == 0:\n",
    "        print(f\"Iteration {n_iter}: Total Loss = {total_loss.item():.4f}\")\n",
    "        # print(f\"Layer Losses: {layer_losses}\") # 디버깅용\n",
    "\n",
    "    n_iter += 1\n",
    "    return total_loss\n",
    "\n",
    "# 최적화 실행 (반복 횟수 지정)\n",
    "num_iterations = 500 # 원하는 반복 횟수를 설정합니다.\n",
    "for i in range(num_iterations):\n",
    "    # L-BFGS는 step() 호출 시마다 클로저를 여러 번 호출할 수 있습니다.\n",
    "    # closure 반환값 기반 처리\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # closure() >> total_loss 반환\n",
    "\n",
    "    # 참고: Adam을 사용한다면, 루프는 다음과 같습니다.\n",
    "    # loss = closure()\n",
    "    # optimizer.step()\n",
    "\n",
    "# 최종 결과 이미지 후처리 (옵션) 이미지 픽셀값 정규화\n",
    "# 생성된 이미지를 [0, 1] 범위로 클리핑하여 픽셀 값을 보정합니다.\n",
    "input_image.data.clamp_(0, 1)\n",
    "\n",
    "# 최적화된 input_image.data가 최종 스타일 트랜스퍼 결과입니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOX/pfU++tDepR6kXyVWDyD",
   "gpuType": "T4",
   "mount_file_id": "1yr1lcbWm4pdpmCzLDOrnCT4Xb3SQajBM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
